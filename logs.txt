
==> Audit <==
|-----------|-----------------------|----------|-------------|---------|---------------------|---------------------|
|  Command  |         Args          | Profile  |    User     | Version |     Start Time      |      End Time       |
|-----------|-----------------------|----------|-------------|---------|---------------------|---------------------|
| start     |                       | minikube | LOTUS\akank | v1.35.0 | 17 Mar 25 23:51 IST | 17 Mar 25 23:59 IST |
| dashboard |                       | minikube | LOTUS\akank | v1.35.0 | 17 Mar 25 23:59 IST |                     |
| tunnel    |                       | minikube | LOTUS\akank | v1.35.0 | 18 Mar 25 00:07 IST |                     |
| tunnel    |                       | minikube | LOTUS\akank | v1.35.0 | 18 Mar 25 00:09 IST |                     |
| addons    | enable ingress-dns    | minikube | LOTUS\akank | v1.35.0 | 18 Mar 25 00:10 IST | 18 Mar 25 00:10 IST |
| addons    | enable ingress        | minikube | LOTUS\akank | v1.35.0 | 18 Mar 25 00:10 IST |                     |
| addons    | list                  | minikube | LOTUS\akank | v1.35.0 | 18 Mar 25 00:11 IST | 18 Mar 25 00:11 IST |
| addons    | enable metrics-server | minikube | LOTUS\akank | v1.35.0 | 18 Mar 25 00:11 IST | 18 Mar 25 00:11 IST |
| dashboard | --url                 | minikube | LOTUS\akank | v1.35.0 | 18 Mar 25 00:18 IST |                     |
|-----------|-----------------------|----------|-------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/03/17 23:51:54
Running on machine: LOTUS
Binary: Built with gc go1.23.4 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0317 23:51:54.541280   21696 out.go:345] Setting OutFile to fd 112 ...
I0317 23:51:54.541280   21696 out.go:358] Setting ErrFile to fd 116...
W0317 23:51:54.552346   21696 root.go:314] Error reading config file at C:\Users\akank\.minikube\config\config.json: open C:\Users\akank\.minikube\config\config.json: The system cannot find the path specified.
I0317 23:51:54.560995   21696 out.go:352] Setting JSON to false
I0317 23:51:54.565731   21696 start.go:129] hostinfo: {"hostname":"LOTUS","uptime":3098,"bootTime":1742232616,"procs":283,"os":"windows","platform":"Microsoft Windows 11 Pro","platformFamily":"Standalone Workstation","platformVersion":"10.0.26100.3476 Build 26100.3476","kernelVersion":"10.0.26100.3476 Build 26100.3476","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"f8d48533-3423-4975-9c07-bfef8eedefca"}
W0317 23:51:54.565731   21696 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0317 23:51:54.567307   21696 out.go:177] 😄  minikube v1.35.0 on Microsoft Windows 11 Pro 10.0.26100.3476 Build 26100.3476
I0317 23:51:54.568271   21696 notify.go:220] Checking for updates...
W0317 23:51:54.568776   21696 preload.go:293] Failed to list preload files: open C:\Users\akank\.minikube\cache\preloaded-tarball: The system cannot find the file specified.
I0317 23:51:54.569296   21696 driver.go:394] Setting default libvirt URI to qemu:///system
I0317 23:51:54.569296   21696 global.go:112] Querying for installed drivers using PATH=C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.8\bin;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.8\libnvvp;C:\Program Files\Python311\Scripts\;C:\Program Files\Python311\;C:\Program Files\Python313\Scripts\;C:\Program Files\Python313\;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\WINDOWS\System32\OpenSSH\;C:\Program Files\dotnet\;C:\Program Files\NVIDIA Corporation\NVIDIA app\NvDLISR;C:\Program Files\Git\cmd;C:\Program Files\Git\mingw64\bin;C:\Program Files\Git\usr\bin;C:\minikube;C:\Users\akank\AppData\Local\Microsoft\WindowsApps;C:\Users\akank\AppData\Local\GitHubDesktop\bin;C:\Users\akank\AppData\Local\Programs\Microsoft VS Code\bin;C:\Users\akank\kube\kubectl.exe;C:\Users\akank\kubectl.exe;C:\minikube\minikube.exe;;C:\your-kubectl-path;C:\Program Files\Microsoft SQL Server\150\Tools\Binn\;C:\Program Files\Microsoft SQL Server\Client SDK\ODBC\170\Tools\Binn\;C:\Program Files (x86)\Windows Kits\10\Windows Performance Toolkit\;C:\Program Files\NVIDIA Corporation\Nsight Compute 2025.1.0\;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Program Files\Cloudflare\Cloudflare WARP\;C:\Users\akank\AppData\Local\Microsoft\WindowsApps;C:\Users\akank\AppData\Local\GitHubDesktop\bin;C:\Users\akank\AppData\Local\Programs\Microsoft VS Code\bin;C:\Users\akank\kube\kubectl.exe;C:\Users\akank\kubectl.exe;C:\minikube\minikube.exe;C:\Program Files\JetBrains\IntelliJ IDEA Community Edition 2024.3.4\bin;C:\Users\akank\AppData\Roaming\Python\Python311\Scripts\wheel.exe;C:\Users\akank\AppData\Roaming\Python\Python311\Scripts\f2py.exe;C:\Users\akank\.dotnet\tools;C:\Users\akank\promtool.exe;C:\Users\akank\helm.exe;C:\Users\akank\.lmstudio\bin
I0317 23:51:54.592700   21696 global.go:133] virtualbox default: true priority: 6, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:unable to find VBoxManage in $PATH Reason: Fix:Install VirtualBox Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/virtualbox/ Version:}
I0317 23:51:54.601582   21696 global.go:133] vmware default: false priority: 5, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "vmrun": executable file not found in %PATH% Reason: Fix:Install vmrun Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
I0317 23:51:54.610530   21696 global.go:133] docker default: true priority: 9, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "docker": executable file not found in %PATH% Reason: Fix:Install Docker Doc:https://minikube.sigs.k8s.io/docs/drivers/docker/ Version:}
I0317 23:51:54.617934   21696 global.go:133] podman default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "podman": executable file not found in %PATH% Reason: Fix:Install Podman Doc:https://minikube.sigs.k8s.io/docs/drivers/podman/ Version:}
I0317 23:51:54.617934   21696 global.go:133] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0317 23:51:55.229116   21696 global.go:133] hyperv default: true priority: 8, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0317 23:51:55.237507   21696 global.go:133] qemu2 default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "qemu-system-x86_64": executable file not found in %PATH% Reason: Fix:Install qemu-system Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/qemu/ Version:}
I0317 23:51:55.238020   21696 driver.go:316] not recommending "ssh" due to default: false
I0317 23:51:55.238020   21696 driver.go:351] Picked: hyperv
I0317 23:51:55.238020   21696 driver.go:352] Alternatives: [ssh]
I0317 23:51:55.238020   21696 driver.go:353] Rejects: [virtualbox vmware docker podman qemu2]
I0317 23:51:55.239043   21696 out.go:177] ✨  Automatically selected the hyperv driver
I0317 23:51:55.239564   21696 start.go:297] selected driver: hyperv
I0317 23:51:55.239564   21696 start.go:901] validating driver "hyperv" against <nil>
I0317 23:51:55.239564   21696 start.go:912] status for hyperv: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0317 23:51:55.240154   21696 start_flags.go:310] no existing cluster config was found, will generate one from the flags 
I0317 23:51:55.264753   21696 start_flags.go:393] Using suggested 3900MB memory alloc based on sys=15881MB, container=0MB
I0317 23:51:55.264753   21696 start_flags.go:929] Wait components to verify : map[apiserver:true system_pods:true]
I0317 23:51:55.264753   21696 cni.go:84] Creating CNI manager for ""
I0317 23:51:55.264753   21696 cni.go:158] "hyperv" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0317 23:51:55.264753   21696 start_flags.go:319] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I0317 23:51:55.264753   21696 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:3900 CPUs:2 DiskSize:20000 Driver:hyperv HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\akank:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0317 23:51:55.264753   21696 iso.go:125] acquiring lock: {Name:mkbc45c396c116d8560dae326e66071bbdde03ef Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0317 23:51:55.266355   21696 out.go:177] 💿  Downloading VM boot image ...
I0317 23:51:55.266355   21696 download.go:108] Downloading: https://storage.googleapis.com/minikube/iso/minikube-v1.35.0-amd64.iso?checksum=file:https://storage.googleapis.com/minikube/iso/minikube-v1.35.0-amd64.iso.sha256 -> C:\Users\akank\.minikube\cache\iso\amd64\minikube-v1.35.0-amd64.iso
I0317 23:52:18.525785   21696 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0317 23:52:18.529933   21696 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0317 23:52:19.333562   21696 preload.go:118] Found remote preload: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.32.0/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0317 23:52:19.333562   21696 cache.go:56] Caching tarball of preloaded images
I0317 23:52:19.334071   21696 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0317 23:52:19.335139   21696 out.go:177] 💾  Downloading Kubernetes v1.32.0 preload ...
I0317 23:52:19.335682   21696 preload.go:236] getting checksum for preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 ...
I0317 23:52:19.981633   21696 download.go:108] Downloading: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.32.0/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4?checksum=md5:4da2ed9bc13e09e8e9b7cf53d01335db -> C:\Users\akank\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0317 23:57:59.475529   21696 preload.go:247] saving checksum for preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 ...
I0317 23:57:59.486218   21696 preload.go:254] verifying checksum of C:\Users\akank\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 ...
I0317 23:57:59.995205   21696 cache.go:59] Finished verifying existence of preloaded tar for v1.32.0 on docker
I0317 23:57:59.995205   21696 profile.go:143] Saving config to C:\Users\akank\.minikube\profiles\minikube\config.json ...
I0317 23:57:59.995205   21696 lock.go:35] WriteFile acquiring C:\Users\akank\.minikube\profiles\minikube\config.json: {Name:mk7f39433c9dc3da993fa2654494215e86993f8b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0317 23:58:00.005665   21696 start.go:360] acquireMachinesLock for minikube: {Name:mk5a5c97e7496429c42724d79f4d64dd9239b9c2 Clock:{} Delay:500ms Timeout:13m0s Cancel:<nil>}
I0317 23:58:00.005665   21696 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I0317 23:58:00.006814   21696 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.35.0-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:3900 CPUs:2 DiskSize:20000 Driver:hyperv HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\akank:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0317 23:58:00.006814   21696 start.go:125] createHost starting for "" (driver="hyperv")
I0317 23:58:00.015333   21696 out.go:235] 🔥  Creating hyperv VM (CPUs=2, Memory=3900MB, Disk=20000MB) ...
I0317 23:58:00.016352   21696 start.go:159] libmachine.API.Create for "minikube" (driver="hyperv")
I0317 23:58:00.016352   21696 client.go:168] LocalClient.Create starting
I0317 23:58:00.016352   21696 main.go:141] libmachine: Creating CA: C:\Users\akank\.minikube\certs\ca.pem
I0317 23:58:00.117512   21696 main.go:141] libmachine: Creating client certificate: C:\Users\akank\.minikube\certs\cert.pem
I0317 23:58:00.182022   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive @(Get-Module -ListAvailable hyper-v).Name | Get-Unique
I0317 23:58:00.407970   21696 main.go:141] libmachine: [stdout =====>] : Hyper-V

I0317 23:58:00.407970   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:00.407970   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive @([Security.Principal.WindowsPrincipal][Security.Principal.WindowsIdentity]::GetCurrent()).IsInRole(([System.Security.Principal.SecurityIdentifier]::new("S-1-5-32-578")))
I0317 23:58:00.561106   21696 main.go:141] libmachine: [stdout =====>] : False

I0317 23:58:00.561106   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:00.561106   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive @([Security.Principal.WindowsPrincipal][Security.Principal.WindowsIdentity]::GetCurrent()).IsInRole([Security.Principal.WindowsBuiltInRole] "Administrator")
I0317 23:58:00.717545   21696 main.go:141] libmachine: [stdout =====>] : True

I0317 23:58:00.717545   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:00.717545   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive [Console]::OutputEncoding = [Text.Encoding]::UTF8; ConvertTo-Json @(Hyper-V\Get-VMSwitch|Select Id, Name, SwitchType|Where-Object {($_.SwitchType -eq 'External') -or ($_.Id -eq 'c08cb7b8-9b3c-408e-8e30-5e16a3aeb444')}|Sort-Object -Property SwitchType)
I0317 23:58:01.837565   21696 main.go:141] libmachine: [stdout =====>] : [
    {
        "Id":  "c08cb7b8-9b3c-408e-8e30-5e16a3aeb444",
        "Name":  "Default Switch",
        "SwitchType":  1
    },
    {
        "Id":  "57d89ba3-dada-4403-8856-4a3264a23f0a",
        "Name":  "MINIKUBESWITCH",
        "SwitchType":  2
    }
]

I0317 23:58:01.837565   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:01.838589   21696 main.go:141] libmachine: Downloading C:\Users\akank\.minikube\cache\boot2docker.iso from file://C:/Users/akank/.minikube/cache/iso/amd64/minikube-v1.35.0-amd64.iso...
I0317 23:58:02.148516   21696 main.go:141] libmachine: Creating SSH key...
I0317 23:58:02.365317   21696 main.go:141] libmachine: Creating VM...
I0317 23:58:02.365317   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive [Console]::OutputEncoding = [Text.Encoding]::UTF8; ConvertTo-Json @(Hyper-V\Get-VMSwitch|Select Id, Name, SwitchType|Where-Object {($_.SwitchType -eq 'External') -or ($_.Id -eq 'c08cb7b8-9b3c-408e-8e30-5e16a3aeb444')}|Sort-Object -Property SwitchType)
I0317 23:58:03.171222   21696 main.go:141] libmachine: [stdout =====>] : [
    {
        "Id":  "c08cb7b8-9b3c-408e-8e30-5e16a3aeb444",
        "Name":  "Default Switch",
        "SwitchType":  1
    },
    {
        "Id":  "57d89ba3-dada-4403-8856-4a3264a23f0a",
        "Name":  "MINIKUBESWITCH",
        "SwitchType":  2
    }
]

I0317 23:58:03.171222   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:03.171222   21696 main.go:141] libmachine: Using switch "Default Switch"
I0317 23:58:03.171222   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive @([Security.Principal.WindowsPrincipal][Security.Principal.WindowsIdentity]::GetCurrent()).IsInRole([Security.Principal.WindowsBuiltInRole] "Administrator")
I0317 23:58:03.354083   21696 main.go:141] libmachine: [stdout =====>] : True

I0317 23:58:03.354083   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:03.354083   21696 main.go:141] libmachine: Creating VHD
I0317 23:58:03.354083   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive Hyper-V\New-VHD -Path 'C:\Users\akank\.minikube\machines\minikube\fixed.vhd' -SizeBytes 10MB -Fixed
I0317 23:58:04.979620   21696 main.go:141] libmachine: [stdout =====>] : 

ComputerName            : LOTUS
Path                    : C:\Users\akank\.minikube\machines\minikube\fixed.vhd
VhdFormat               : VHD
VhdType                 : Fixed
FileSize                : 10486272
Size                    : 10485760
MinimumSize             : 
LogicalSectorSize       : 512
PhysicalSectorSize      : 512
BlockSize               : 0
ParentPath              : 
DiskIdentifier          : C3B02543-E339-4954-976D-D90CAEB30C1D
FragmentationPercentage : 0
Alignment               : 1
Attached                : False
DiskNumber              : 
IsPMEMCompatible        : False
AddressAbstractionType  : None
Number                  : 




I0317 23:58:04.979620   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:04.979620   21696 main.go:141] libmachine: Writing magic tar header
I0317 23:58:04.979620   21696 main.go:141] libmachine: Writing SSH key tar header
I0317 23:58:04.986697   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive Hyper-V\Convert-VHD -Path 'C:\Users\akank\.minikube\machines\minikube\fixed.vhd' -DestinationPath 'C:\Users\akank\.minikube\machines\minikube\disk.vhd' -VHDType Dynamic -DeleteSource
I0317 23:58:06.426435   21696 main.go:141] libmachine: [stdout =====>] : 
I0317 23:58:06.426435   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:06.426435   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive Hyper-V\Resize-VHD -Path 'C:\Users\akank\.minikube\machines\minikube\disk.vhd' -SizeBytes 20000MB
I0317 23:58:07.356234   21696 main.go:141] libmachine: [stdout =====>] : 
I0317 23:58:07.356234   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:07.356234   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive Hyper-V\New-VM minikube -Path 'C:\Users\akank\.minikube\machines\minikube' -SwitchName 'Default Switch' -MemoryStartupBytes 3900MB
I0317 23:58:09.467139   21696 main.go:141] libmachine: [stdout =====>] : 
Name     State CPUUsage(%) MemoryAssigned(M) Uptime   Status             Version
----     ----- ----------- ----------------- ------   ------             -------
minikube Off   0           0                 00:00:00 Operating normally 12.0   



I0317 23:58:09.467139   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:09.467139   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive Hyper-V\Set-VMMemory -VMName minikube -DynamicMemoryEnabled $false
I0317 23:58:09.938086   21696 main.go:141] libmachine: [stdout =====>] : 
I0317 23:58:09.938086   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:09.938086   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive Hyper-V\Set-VMProcessor minikube -Count 2
I0317 23:58:10.366766   21696 main.go:141] libmachine: [stdout =====>] : 
I0317 23:58:10.366766   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:10.366766   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive Hyper-V\Set-VMDvdDrive -VMName minikube -Path 'C:\Users\akank\.minikube\machines\minikube\boot2docker.iso'
I0317 23:58:11.009260   21696 main.go:141] libmachine: [stdout =====>] : 
I0317 23:58:11.009260   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:11.009260   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive Hyper-V\Add-VMHardDiskDrive -VMName minikube -Path 'C:\Users\akank\.minikube\machines\minikube\disk.vhd'
I0317 23:58:11.718346   21696 main.go:141] libmachine: [stdout =====>] : 
I0317 23:58:11.718346   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:11.718346   21696 main.go:141] libmachine: Starting VM...
I0317 23:58:11.718346   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive Hyper-V\Start-VM minikube
I0317 23:58:16.633126   21696 main.go:141] libmachine: [stdout =====>] : 
I0317 23:58:16.633126   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:16.633126   21696 main.go:141] libmachine: Waiting for host to start...
I0317 23:58:16.633126   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0317 23:58:17.175903   21696 main.go:141] libmachine: [stdout =====>] : Running

I0317 23:58:17.175903   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:17.175903   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I0317 23:58:17.950734   21696 main.go:141] libmachine: [stdout =====>] : 
I0317 23:58:17.950734   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:18.951161   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0317 23:58:19.390429   21696 main.go:141] libmachine: [stdout =====>] : Running

I0317 23:58:19.390429   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:19.390429   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I0317 23:58:20.040917   21696 main.go:141] libmachine: [stdout =====>] : 
I0317 23:58:20.040917   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:21.041356   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0317 23:58:21.518425   21696 main.go:141] libmachine: [stdout =====>] : Running

I0317 23:58:21.518425   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:21.518425   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I0317 23:58:22.270601   21696 main.go:141] libmachine: [stdout =====>] : 
I0317 23:58:22.270601   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:23.271004   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0317 23:58:23.971118   21696 main.go:141] libmachine: [stdout =====>] : Running

I0317 23:58:23.971118   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:23.971118   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I0317 23:58:24.932315   21696 main.go:141] libmachine: [stdout =====>] : 192.168.181.188

I0317 23:58:24.932315   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:24.932315   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0317 23:58:25.674046   21696 main.go:141] libmachine: [stdout =====>] : Running

I0317 23:58:25.674046   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:25.674551   21696 machine.go:93] provisionDockerMachine start ...
I0317 23:58:25.675111   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0317 23:58:26.222962   21696 main.go:141] libmachine: [stdout =====>] : Running

I0317 23:58:26.222962   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:26.222962   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I0317 23:58:27.038166   21696 main.go:141] libmachine: [stdout =====>] : 192.168.181.188

I0317 23:58:27.038166   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:27.040262   21696 main.go:141] libmachine: Using SSH client type: native
I0317 23:58:27.052054   21696 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x575360] 0x577ea0 <nil>  [] 0s} 192.168.181.188 22 <nil> <nil>}
I0317 23:58:27.052054   21696 main.go:141] libmachine: About to run SSH command:
hostname
I0317 23:58:27.161881   21696 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0317 23:58:27.162913   21696 buildroot.go:166] provisioning hostname "minikube"
I0317 23:58:27.163444   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0317 23:58:27.696993   21696 main.go:141] libmachine: [stdout =====>] : Running

I0317 23:58:27.696993   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:27.698653   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I0317 23:58:28.360509   21696 main.go:141] libmachine: [stdout =====>] : 192.168.181.188

I0317 23:58:28.360509   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:28.362584   21696 main.go:141] libmachine: Using SSH client type: native
I0317 23:58:28.363117   21696 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x575360] 0x577ea0 <nil>  [] 0s} 192.168.181.188 22 <nil> <nil>}
I0317 23:58:28.363117   21696 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0317 23:58:28.472376   21696 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0317 23:58:28.472882   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0317 23:58:28.933069   21696 main.go:141] libmachine: [stdout =====>] : Running

I0317 23:58:28.933069   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:28.933069   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I0317 23:58:29.583382   21696 main.go:141] libmachine: [stdout =====>] : 192.168.181.188

I0317 23:58:29.583382   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:29.585761   21696 main.go:141] libmachine: Using SSH client type: native
I0317 23:58:29.586279   21696 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x575360] 0x577ea0 <nil>  [] 0s} 192.168.181.188 22 <nil> <nil>}
I0317 23:58:29.586279   21696 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0317 23:58:29.680593   21696 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0317 23:58:29.681623   21696 buildroot.go:172] set auth options {CertDir:C:\Users\akank\.minikube CaCertPath:C:\Users\akank\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\akank\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\akank\.minikube\machines\server.pem ServerKeyPath:C:\Users\akank\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\akank\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\akank\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\akank\.minikube}
I0317 23:58:29.681623   21696 buildroot.go:174] setting up certificates
I0317 23:58:29.681623   21696 provision.go:84] configureAuth start
I0317 23:58:29.681623   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0317 23:58:30.179184   21696 main.go:141] libmachine: [stdout =====>] : Running

I0317 23:58:30.179184   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:30.179184   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I0317 23:58:30.931632   21696 main.go:141] libmachine: [stdout =====>] : 192.168.181.188

I0317 23:58:30.931632   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:30.931632   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0317 23:58:31.683201   21696 main.go:141] libmachine: [stdout =====>] : Running

I0317 23:58:31.683201   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:31.683201   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I0317 23:58:32.712471   21696 main.go:141] libmachine: [stdout =====>] : 192.168.181.188

I0317 23:58:32.712471   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:32.713168   21696 provision.go:143] copyHostCerts
I0317 23:58:32.715454   21696 exec_runner.go:151] cp: C:\Users\akank\.minikube\certs\ca.pem --> C:\Users\akank\.minikube/ca.pem (1074 bytes)
I0317 23:58:32.717048   21696 exec_runner.go:151] cp: C:\Users\akank\.minikube\certs\cert.pem --> C:\Users\akank\.minikube/cert.pem (1119 bytes)
I0317 23:58:32.718875   21696 exec_runner.go:151] cp: C:\Users\akank\.minikube\certs\key.pem --> C:\Users\akank\.minikube/key.pem (1675 bytes)
I0317 23:58:32.721040   21696 provision.go:117] generating server cert: C:\Users\akank\.minikube\machines\server.pem ca-key=C:\Users\akank\.minikube\certs\ca.pem private-key=C:\Users\akank\.minikube\certs\ca-key.pem org=akank.minikube san=[127.0.0.1 192.168.181.188 localhost minikube]
I0317 23:58:32.942833   21696 provision.go:177] copyRemoteCerts
I0317 23:58:32.945340   21696 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0317 23:58:32.945340   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0317 23:58:34.175578   21696 main.go:141] libmachine: [stdout =====>] : Running

I0317 23:58:34.175578   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:34.175578   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I0317 23:58:35.345781   21696 main.go:141] libmachine: [stdout =====>] : 192.168.181.188

I0317 23:58:35.345781   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:35.346287   21696 sshutil.go:53] new ssh client: &{IP:192.168.181.188 Port:22 SSHKeyPath:C:\Users\akank\.minikube\machines\minikube\id_rsa Username:docker}
I0317 23:58:35.435417   21696 ssh_runner.go:235] Completed: sudo mkdir -p /etc/docker /etc/docker /etc/docker: (2.4900774s)
I0317 23:58:35.454149   21696 ssh_runner.go:362] scp C:\Users\akank\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0317 23:58:35.492122   21696 ssh_runner.go:362] scp C:\Users\akank\.minikube\machines\server.pem --> /etc/docker/server.pem (1176 bytes)
I0317 23:58:35.525991   21696 ssh_runner.go:362] scp C:\Users\akank\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0317 23:58:35.555765   21696 provision.go:87] duration metric: took 5.8735893s to configureAuth
I0317 23:58:35.555765   21696 buildroot.go:189] setting minikube options for container-runtime
I0317 23:58:35.558021   21696 config.go:182] Loaded profile config "minikube": Driver=hyperv, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0317 23:58:35.558021   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0317 23:58:36.147561   21696 main.go:141] libmachine: [stdout =====>] : Running

I0317 23:58:36.147561   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:36.147561   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I0317 23:58:37.219568   21696 main.go:141] libmachine: [stdout =====>] : 192.168.181.188

I0317 23:58:37.219568   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:37.224971   21696 main.go:141] libmachine: Using SSH client type: native
I0317 23:58:37.225354   21696 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x575360] 0x577ea0 <nil>  [] 0s} 192.168.181.188 22 <nil> <nil>}
I0317 23:58:37.225354   21696 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0317 23:58:37.342897   21696 main.go:141] libmachine: SSH cmd err, output: <nil>: tmpfs

I0317 23:58:37.343403   21696 buildroot.go:70] root file system type: tmpfs
I0317 23:58:37.346020   21696 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0317 23:58:37.347063   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0317 23:58:38.014383   21696 main.go:141] libmachine: [stdout =====>] : Running

I0317 23:58:38.014383   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:38.014383   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I0317 23:58:38.882513   21696 main.go:141] libmachine: [stdout =====>] : 192.168.181.188

I0317 23:58:38.882513   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:38.884768   21696 main.go:141] libmachine: Using SSH client type: native
I0317 23:58:38.884768   21696 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x575360] 0x577ea0 <nil>  [] 0s} 192.168.181.188 22 <nil> <nil>}
I0317 23:58:38.884768   21696 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=hyperv --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0317 23:58:38.998624   21696 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=hyperv --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0317 23:58:38.998624   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0317 23:58:39.483097   21696 main.go:141] libmachine: [stdout =====>] : Running

I0317 23:58:39.483097   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:39.483097   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I0317 23:58:40.112656   21696 main.go:141] libmachine: [stdout =====>] : 192.168.181.188

I0317 23:58:40.112656   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:40.115260   21696 main.go:141] libmachine: Using SSH client type: native
I0317 23:58:40.115260   21696 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x575360] 0x577ea0 <nil>  [] 0s} 192.168.181.188 22 <nil> <nil>}
I0317 23:58:40.115260   21696 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0317 23:58:41.637741   21696 main.go:141] libmachine: SSH cmd err, output: <nil>: diff: can't stat '/lib/systemd/system/docker.service': No such file or directory
Created symlink /etc/systemd/system/multi-user.target.wants/docker.service → /usr/lib/systemd/system/docker.service.

I0317 23:58:41.638501   21696 machine.go:96] duration metric: took 15.96319s to provisionDockerMachine
I0317 23:58:41.639039   21696 client.go:171] duration metric: took 41.6226874s to LocalClient.Create
I0317 23:58:41.639396   21696 start.go:167] duration metric: took 41.6230445s to libmachine.API.Create "minikube"
I0317 23:58:41.639396   21696 start.go:293] postStartSetup for "minikube" (driver="hyperv")
I0317 23:58:41.640017   21696 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0317 23:58:41.642103   21696 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0317 23:58:41.642614   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0317 23:58:42.130133   21696 main.go:141] libmachine: [stdout =====>] : Running

I0317 23:58:42.130133   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:42.130133   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I0317 23:58:42.791991   21696 main.go:141] libmachine: [stdout =====>] : 192.168.181.188

I0317 23:58:42.791991   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:42.791991   21696 sshutil.go:53] new ssh client: &{IP:192.168.181.188 Port:22 SSHKeyPath:C:\Users\akank\.minikube\machines\minikube\id_rsa Username:docker}
I0317 23:58:42.875818   21696 ssh_runner.go:235] Completed: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs: (1.2337148s)
I0317 23:58:42.879469   21696 ssh_runner.go:195] Run: cat /etc/os-release
I0317 23:58:42.882983   21696 info.go:137] Remote host: Buildroot 2023.02.9
I0317 23:58:42.883498   21696 filesync.go:126] Scanning C:\Users\akank\.minikube\addons for local assets ...
I0317 23:58:42.884268   21696 filesync.go:126] Scanning C:\Users\akank\.minikube\files for local assets ...
I0317 23:58:42.884268   21696 start.go:296] duration metric: took 1.2448716s for postStartSetup
I0317 23:58:42.886325   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0317 23:58:43.375993   21696 main.go:141] libmachine: [stdout =====>] : Running

I0317 23:58:43.375993   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:43.375993   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I0317 23:58:44.013543   21696 main.go:141] libmachine: [stdout =====>] : 192.168.181.188

I0317 23:58:44.013543   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:44.014566   21696 profile.go:143] Saving config to C:\Users\akank\.minikube\profiles\minikube\config.json ...
I0317 23:58:44.016100   21696 start.go:128] duration metric: took 44.0087715s to createHost
I0317 23:58:44.016621   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0317 23:58:44.508484   21696 main.go:141] libmachine: [stdout =====>] : Running

I0317 23:58:44.508484   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:44.508484   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I0317 23:58:45.119646   21696 main.go:141] libmachine: [stdout =====>] : 192.168.181.188

I0317 23:58:45.119646   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:45.121701   21696 main.go:141] libmachine: Using SSH client type: native
I0317 23:58:45.121701   21696 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x575360] 0x577ea0 <nil>  [] 0s} 192.168.181.188 22 <nil> <nil>}
I0317 23:58:45.121701   21696 main.go:141] libmachine: About to run SSH command:
date +%s.%N
I0317 23:58:45.223589   21696 main.go:141] libmachine: SSH cmd err, output: <nil>: 1742236125.209111805

I0317 23:58:45.223589   21696 fix.go:216] guest clock: 1742236125.209111805
I0317 23:58:45.223589   21696 fix.go:229] Guest: 2025-03-17 23:58:45.209111805 +0530 IST Remote: 2025-03-17 23:58:44.0161004 +0530 IST m=+409.519718001 (delta=1.193011405s)
I0317 23:58:45.233593   21696 fix.go:200] guest clock delta is within tolerance: 1.193011405s
I0317 23:58:45.233593   21696 start.go:83] releasing machines lock for "minikube", held for 45.2279286s
I0317 23:58:45.233593   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0317 23:58:45.686149   21696 main.go:141] libmachine: [stdout =====>] : Running

I0317 23:58:45.686149   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:45.686149   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I0317 23:58:46.370171   21696 main.go:141] libmachine: [stdout =====>] : 192.168.181.188

I0317 23:58:46.370171   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:46.375391   21696 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I0317 23:58:46.375915   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0317 23:58:46.378614   21696 ssh_runner.go:195] Run: cat /version.json
I0317 23:58:46.378614   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0317 23:58:47.148918   21696 main.go:141] libmachine: [stdout =====>] : Running

I0317 23:58:47.148918   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:47.148918   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I0317 23:58:47.148918   21696 main.go:141] libmachine: [stdout =====>] : Running

I0317 23:58:47.148918   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:47.148918   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I0317 23:58:47.950223   21696 main.go:141] libmachine: [stdout =====>] : 192.168.181.188

I0317 23:58:47.950223   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:47.950223   21696 sshutil.go:53] new ssh client: &{IP:192.168.181.188 Port:22 SSHKeyPath:C:\Users\akank\.minikube\machines\minikube\id_rsa Username:docker}
I0317 23:58:47.954513   21696 main.go:141] libmachine: [stdout =====>] : 192.168.181.188

I0317 23:58:47.954513   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:58:47.954513   21696 sshutil.go:53] new ssh client: &{IP:192.168.181.188 Port:22 SSHKeyPath:C:\Users\akank\.minikube\machines\minikube\id_rsa Username:docker}
I0317 23:58:48.026282   21696 ssh_runner.go:235] Completed: curl.exe -sS -m 2 https://registry.k8s.io/: (1.6508907s)
I0317 23:58:48.026282   21696 ssh_runner.go:235] Completed: cat /version.json: (1.6476676s)
W0317 23:58:48.026282   21696 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I0317 23:58:48.038207   21696 ssh_runner.go:195] Run: systemctl --version
I0317 23:58:48.052619   21696 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
W0317 23:58:48.059623   21696 cni.go:209] loopback cni configuration skipped: "/etc/cni/net.d/*loopback.conf*" not found
I0317 23:58:48.061709   21696 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0317 23:58:48.077557   21696 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0317 23:58:48.077557   21696 start.go:495] detecting cgroup driver to use...
I0317 23:58:48.079116   21696 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0317 23:58:48.094242   21696 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0317 23:58:48.106816   21696 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0317 23:58:48.115136   21696 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0317 23:58:48.118622   21696 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0317 23:58:48.130023   21696 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0317 23:58:48.142377   21696 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0317 23:58:48.153425   21696 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0317 23:58:48.165366   21696 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0317 23:58:48.175391   21696 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0317 23:58:48.189056   21696 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0317 23:58:48.198921   21696 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0317 23:58:48.207526   21696 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0317 23:58:48.214615   21696 crio.go:166] couldn't verify netfilter by "sudo sysctl net.bridge.bridge-nf-call-iptables" which might be okay. error: sudo sysctl net.bridge.bridge-nf-call-iptables: Process exited with status 255
stdout:

stderr:
sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-iptables: No such file or directory
I0317 23:58:48.216192   21696 ssh_runner.go:195] Run: sudo modprobe br_netfilter
I0317 23:58:48.224970   21696 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0317 23:58:48.234229   21696 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0317 23:58:48.313602   21696 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0317 23:58:48.328445   21696 start.go:495] detecting cgroup driver to use...
I0317 23:58:48.332669   21696 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0317 23:58:48.343976   21696 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I0317 23:58:48.358047   21696 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I0317 23:58:48.371882   21696 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I0317 23:58:48.382065   21696 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0317 23:58:48.395412   21696 ssh_runner.go:195] Run: sudo systemctl stop -f crio
I0317 23:58:48.421618   21696 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0317 23:58:48.430359   21696 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0317 23:58:48.448827   21696 ssh_runner.go:195] Run: which cri-dockerd
I0317 23:58:48.454023   21696 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0317 23:58:48.460249   21696 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0317 23:58:48.472475   21696 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
W0317 23:58:48.513651   21696 out.go:270] ❗  Failing to connect to https://registry.k8s.io/ from inside the minikube VM
W0317 23:58:48.514179   21696 out.go:270] 💡  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0317 23:58:48.569412   21696 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0317 23:58:48.656446   21696 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0317 23:58:48.659077   21696 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0317 23:58:48.671098   21696 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0317 23:58:48.758253   21696 ssh_runner.go:195] Run: sudo systemctl restart docker
I0317 23:58:50.946064   21696 ssh_runner.go:235] Completed: sudo systemctl restart docker: (2.1878104s)
I0317 23:58:50.947674   21696 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0317 23:58:50.957752   21696 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0317 23:58:50.968166   21696 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0317 23:58:51.053026   21696 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0317 23:58:51.139969   21696 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0317 23:58:51.221078   21696 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0317 23:58:51.237046   21696 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0317 23:58:51.248309   21696 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0317 23:58:51.336059   21696 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0317 23:58:51.375431   21696 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0317 23:58:51.380378   21696 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0317 23:58:51.385338   21696 start.go:563] Will wait 60s for crictl version
I0317 23:58:51.389130   21696 ssh_runner.go:195] Run: which crictl
I0317 23:58:51.394225   21696 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0317 23:58:51.417358   21696 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.4.0
RuntimeApiVersion:  v1
I0317 23:58:51.426170   21696 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0317 23:58:51.449276   21696 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0317 23:58:51.465112   21696 out.go:235] 🐳  Preparing Kubernetes v1.32.0 on Docker 27.4.0 ...
I0317 23:58:51.466679   21696 ip.go:176] getIPForInterface: searching for "vEthernet (Default Switch)"
I0317 23:58:51.469753   21696 ip.go:190] "vEthernet (MINIKUBESWITCH)" does not match prefix "vEthernet (Default Switch)"
I0317 23:58:51.469753   21696 ip.go:190] "Local Area Connection* 1" does not match prefix "vEthernet (Default Switch)"
I0317 23:58:51.469753   21696 ip.go:190] "Local Area Connection* 2" does not match prefix "vEthernet (Default Switch)"
I0317 23:58:51.469753   21696 ip.go:190] "WiFi" does not match prefix "vEthernet (Default Switch)"
I0317 23:58:51.469753   21696 ip.go:190] "Bluetooth Network Connection" does not match prefix "vEthernet (Default Switch)"
I0317 23:58:51.469753   21696 ip.go:190] "Loopback Pseudo-Interface 1" does not match prefix "vEthernet (Default Switch)"
I0317 23:58:51.469753   21696 ip.go:185] found prefix matching interface for "vEthernet (Default Switch)": "vEthernet (Default Switch)"
I0317 23:58:51.469753   21696 ip.go:211] Found interface: {Index:27 MTU:1500 Name:vEthernet (Default Switch) HardwareAddr:00:15:5d:75:60:00 Flags:up|broadcast|multicast|running}
I0317 23:58:51.472852   21696 ip.go:214] interface addr: fe80::924a:7ab8:3853:21dc/64
I0317 23:58:51.472852   21696 ip.go:214] interface addr: 192.168.176.1/20
I0317 23:58:51.477036   21696 ssh_runner.go:195] Run: grep 192.168.176.1	host.minikube.internal$ /etc/hosts
I0317 23:58:51.480289   21696 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.176.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0317 23:58:51.488834   21696 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.35.0-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:3900 CPUs:2 DiskSize:20000 Driver:hyperv HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.181.188 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\akank:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0317 23:58:51.489872   21696 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0317 23:58:51.500434   21696 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0317 23:58:51.512862   21696 docker.go:689] Got preloaded images: 
I0317 23:58:51.512862   21696 docker.go:695] registry.k8s.io/kube-apiserver:v1.32.0 wasn't preloaded
I0317 23:58:51.514411   21696 ssh_runner.go:195] Run: sudo cat /var/lib/docker/image/overlay2/repositories.json
I0317 23:58:51.526818   21696 ssh_runner.go:195] Run: which lz4
I0317 23:58:51.534272   21696 ssh_runner.go:195] Run: stat -c "%s %y" /preloaded.tar.lz4
I0317 23:58:51.537162   21696 ssh_runner.go:352] existence check for /preloaded.tar.lz4: stat -c "%s %y" /preloaded.tar.lz4: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/preloaded.tar.lz4': No such file or directory
I0317 23:58:51.537162   21696 ssh_runner.go:362] scp C:\Users\akank\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 --> /preloaded.tar.lz4 (349777613 bytes)
I0317 23:58:52.560482   21696 docker.go:653] duration metric: took 1.029886s to copy over tarball
I0317 23:58:52.560482   21696 ssh_runner.go:195] Run: sudo tar --xattrs --xattrs-include security.capability -I lz4 -C /var -xf /preloaded.tar.lz4
I0317 23:58:54.923916   21696 ssh_runner.go:235] Completed: sudo tar --xattrs --xattrs-include security.capability -I lz4 -C /var -xf /preloaded.tar.lz4: (2.3634338s)
I0317 23:58:54.923916   21696 ssh_runner.go:146] rm: /preloaded.tar.lz4
I0317 23:58:54.967299   21696 ssh_runner.go:195] Run: sudo cat /var/lib/docker/image/overlay2/repositories.json
I0317 23:58:54.992419   21696 ssh_runner.go:362] scp memory --> /var/lib/docker/image/overlay2/repositories.json (2631 bytes)
I0317 23:58:55.010155   21696 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0317 23:58:55.146611   21696 ssh_runner.go:195] Run: sudo systemctl restart docker
I0317 23:58:57.912129   21696 ssh_runner.go:235] Completed: sudo systemctl restart docker: (2.7655186s)
I0317 23:58:57.928801   21696 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0317 23:58:57.952131   21696 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0317 23:58:57.952671   21696 cache_images.go:84] Images are preloaded, skipping loading
I0317 23:58:57.953248   21696 kubeadm.go:934] updating node { 192.168.181.188 8443 v1.32.0 docker true true} ...
I0317 23:58:57.957446   21696 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.181.188

[Install]
 config:
{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0317 23:58:57.974666   21696 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0317 23:58:58.033027   21696 cni.go:84] Creating CNI manager for ""
I0317 23:58:58.033027   21696 cni.go:158] "hyperv" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0317 23:58:58.033551   21696 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0317 23:58:58.033551   21696 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.181.188 APIServerPort:8443 KubernetesVersion:v1.32.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.181.188"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.181.188 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0317 23:58:58.034073   21696 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.181.188
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.181.188"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.181.188"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.32.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0317 23:58:58.037197   21696 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.32.0
I0317 23:58:58.048468   21696 binaries.go:44] Found k8s binaries, skipping transfer
I0317 23:58:58.051702   21696 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0317 23:58:58.060263   21696 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (310 bytes)
I0317 23:58:58.082223   21696 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0317 23:58:58.103870   21696 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2295 bytes)
I0317 23:58:58.127753   21696 ssh_runner.go:195] Run: grep 192.168.181.188	control-plane.minikube.internal$ /etc/hosts
I0317 23:58:58.132509   21696 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.181.188	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0317 23:58:58.148157   21696 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0317 23:58:58.287389   21696 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0317 23:58:58.306722   21696 certs.go:68] Setting up C:\Users\akank\.minikube\profiles\minikube for IP: 192.168.181.188
I0317 23:58:58.306722   21696 certs.go:194] generating shared ca certs ...
I0317 23:58:58.307785   21696 certs.go:226] acquiring lock for ca certs: {Name:mk9e911734d16671aff4b7da1177f5d4946261eb Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0317 23:58:58.308306   21696 certs.go:240] generating "minikubeCA" ca cert: C:\Users\akank\.minikube\ca.key
I0317 23:58:58.585447   21696 crypto.go:156] Writing cert to C:\Users\akank\.minikube\ca.crt ...
I0317 23:58:58.585447   21696 lock.go:35] WriteFile acquiring C:\Users\akank\.minikube\ca.crt: {Name:mk9957693c40ce08caf5b390811af1bf535fbd4e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0317 23:58:58.585447   21696 crypto.go:164] Writing key to C:\Users\akank\.minikube\ca.key ...
I0317 23:58:58.585447   21696 lock.go:35] WriteFile acquiring C:\Users\akank\.minikube\ca.key: {Name:mk74a18a06a0016a72b6881963dd23708137a2f6 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0317 23:58:58.585447   21696 certs.go:240] generating "proxyClientCA" ca cert: C:\Users\akank\.minikube\proxy-client-ca.key
I0317 23:58:58.648318   21696 crypto.go:156] Writing cert to C:\Users\akank\.minikube\proxy-client-ca.crt ...
I0317 23:58:58.648318   21696 lock.go:35] WriteFile acquiring C:\Users\akank\.minikube\proxy-client-ca.crt: {Name:mk20b8a9cd7799a559d2a59e33f13b212c832b7b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0317 23:58:58.648318   21696 crypto.go:164] Writing key to C:\Users\akank\.minikube\proxy-client-ca.key ...
I0317 23:58:58.648318   21696 lock.go:35] WriteFile acquiring C:\Users\akank\.minikube\proxy-client-ca.key: {Name:mk9f331356c8ff6479630016df3f1dca77d85bac Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0317 23:58:58.648318   21696 certs.go:256] generating profile certs ...
I0317 23:58:58.655383   21696 certs.go:363] generating signed profile cert for "minikube-user": C:\Users\akank\.minikube\profiles\minikube\client.key
I0317 23:58:58.655383   21696 crypto.go:68] Generating cert C:\Users\akank\.minikube\profiles\minikube\client.crt with IP's: []
I0317 23:58:58.817721   21696 crypto.go:156] Writing cert to C:\Users\akank\.minikube\profiles\minikube\client.crt ...
I0317 23:58:58.817721   21696 lock.go:35] WriteFile acquiring C:\Users\akank\.minikube\profiles\minikube\client.crt: {Name:mk0ac5d73bebe49232bfc9368b60c0c721540f8b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0317 23:58:58.818721   21696 crypto.go:164] Writing key to C:\Users\akank\.minikube\profiles\minikube\client.key ...
I0317 23:58:58.818721   21696 lock.go:35] WriteFile acquiring C:\Users\akank\.minikube\profiles\minikube\client.key: {Name:mk747617354c68c66d3c211bf2b67f98a53ead48 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0317 23:58:58.818721   21696 certs.go:363] generating signed profile cert for "minikube": C:\Users\akank\.minikube\profiles\minikube\apiserver.key.04d894a5
I0317 23:58:58.818721   21696 crypto.go:68] Generating cert C:\Users\akank\.minikube\profiles\minikube\apiserver.crt.04d894a5 with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.181.188]
I0317 23:58:59.015451   21696 crypto.go:156] Writing cert to C:\Users\akank\.minikube\profiles\minikube\apiserver.crt.04d894a5 ...
I0317 23:58:59.015451   21696 lock.go:35] WriteFile acquiring C:\Users\akank\.minikube\profiles\minikube\apiserver.crt.04d894a5: {Name:mk9fffd76cf78cc16dec74b0ae227ab6c8dc4a3e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0317 23:58:59.015451   21696 crypto.go:164] Writing key to C:\Users\akank\.minikube\profiles\minikube\apiserver.key.04d894a5 ...
I0317 23:58:59.015451   21696 lock.go:35] WriteFile acquiring C:\Users\akank\.minikube\profiles\minikube\apiserver.key.04d894a5: {Name:mk0a69674954efc9dddcda7eb86ae8c4f9b9089d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0317 23:58:59.020527   21696 certs.go:381] copying C:\Users\akank\.minikube\profiles\minikube\apiserver.crt.04d894a5 -> C:\Users\akank\.minikube\profiles\minikube\apiserver.crt
I0317 23:58:59.029447   21696 certs.go:385] copying C:\Users\akank\.minikube\profiles\minikube\apiserver.key.04d894a5 -> C:\Users\akank\.minikube\profiles\minikube\apiserver.key
I0317 23:58:59.034899   21696 certs.go:363] generating signed profile cert for "aggregator": C:\Users\akank\.minikube\profiles\minikube\proxy-client.key
I0317 23:58:59.034899   21696 crypto.go:68] Generating cert C:\Users\akank\.minikube\profiles\minikube\proxy-client.crt with IP's: []
I0317 23:58:59.311260   21696 crypto.go:156] Writing cert to C:\Users\akank\.minikube\profiles\minikube\proxy-client.crt ...
I0317 23:58:59.311260   21696 lock.go:35] WriteFile acquiring C:\Users\akank\.minikube\profiles\minikube\proxy-client.crt: {Name:mk0fbc193e6cd002796ac1de858be800b276d15d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0317 23:58:59.312766   21696 crypto.go:164] Writing key to C:\Users\akank\.minikube\profiles\minikube\proxy-client.key ...
I0317 23:58:59.312766   21696 lock.go:35] WriteFile acquiring C:\Users\akank\.minikube\profiles\minikube\proxy-client.key: {Name:mk4b1490bad8c965da30a1377eb7ab7379ac9db4 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0317 23:58:59.324347   21696 certs.go:484] found cert: C:\Users\akank\.minikube\certs\ca-key.pem (1679 bytes)
I0317 23:58:59.324997   21696 certs.go:484] found cert: C:\Users\akank\.minikube\certs\ca.pem (1074 bytes)
I0317 23:58:59.324997   21696 certs.go:484] found cert: C:\Users\akank\.minikube\certs\cert.pem (1119 bytes)
I0317 23:58:59.325509   21696 certs.go:484] found cert: C:\Users\akank\.minikube\certs\key.pem (1675 bytes)
I0317 23:58:59.337183   21696 ssh_runner.go:362] scp C:\Users\akank\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0317 23:58:59.364107   21696 ssh_runner.go:362] scp C:\Users\akank\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0317 23:58:59.388835   21696 ssh_runner.go:362] scp C:\Users\akank\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0317 23:58:59.416060   21696 ssh_runner.go:362] scp C:\Users\akank\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0317 23:58:59.446120   21696 ssh_runner.go:362] scp C:\Users\akank\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0317 23:58:59.471468   21696 ssh_runner.go:362] scp C:\Users\akank\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0317 23:58:59.500872   21696 ssh_runner.go:362] scp C:\Users\akank\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0317 23:58:59.533611   21696 ssh_runner.go:362] scp C:\Users\akank\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0317 23:58:59.565763   21696 ssh_runner.go:362] scp C:\Users\akank\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0317 23:58:59.603235   21696 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0317 23:58:59.630546   21696 ssh_runner.go:195] Run: openssl version
I0317 23:58:59.640411   21696 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0317 23:58:59.660647   21696 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0317 23:58:59.666052   21696 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Mar 17 18:28 /usr/share/ca-certificates/minikubeCA.pem
I0317 23:58:59.671647   21696 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0317 23:58:59.682660   21696 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0317 23:58:59.703095   21696 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0317 23:58:59.708747   21696 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0317 23:58:59.709258   21696 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.35.0-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:3900 CPUs:2 DiskSize:20000 Driver:hyperv HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.181.188 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\akank:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0317 23:58:59.725691   21696 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0317 23:58:59.746295   21696 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0317 23:58:59.761562   21696 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0317 23:58:59.774139   21696 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0317 23:58:59.784440   21696 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0317 23:58:59.784440   21696 kubeadm.go:157] found existing configuration files:

I0317 23:58:59.787086   21696 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0317 23:58:59.797308   21696 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0317 23:58:59.806823   21696 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0317 23:58:59.819205   21696 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0317 23:58:59.829055   21696 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0317 23:58:59.831746   21696 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0317 23:58:59.845437   21696 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0317 23:58:59.863714   21696 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0317 23:58:59.867585   21696 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0317 23:58:59.882121   21696 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0317 23:58:59.889416   21696 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0317 23:58:59.891212   21696 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0317 23:58:59.898680   21696 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.32.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem"
I0317 23:58:59.974091   21696 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0317 23:59:07.890427   21696 kubeadm.go:310] [init] Using Kubernetes version: v1.32.0
I0317 23:59:07.890427   21696 kubeadm.go:310] [preflight] Running pre-flight checks
I0317 23:59:07.890427   21696 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I0317 23:59:07.890427   21696 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0317 23:59:07.890944   21696 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I0317 23:59:07.890944   21696 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0317 23:59:07.891483   21696 out.go:235]     ▪ Generating certificates and keys ...
I0317 23:59:07.892016   21696 kubeadm.go:310] [certs] Using existing ca certificate authority
I0317 23:59:07.892016   21696 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I0317 23:59:07.892551   21696 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I0317 23:59:07.892551   21696 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I0317 23:59:07.892551   21696 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I0317 23:59:07.892551   21696 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I0317 23:59:07.892551   21696 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I0317 23:59:07.892551   21696 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.181.188 127.0.0.1 ::1]
I0317 23:59:07.892551   21696 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I0317 23:59:07.892551   21696 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.181.188 127.0.0.1 ::1]
I0317 23:59:07.893074   21696 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I0317 23:59:07.893074   21696 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I0317 23:59:07.893074   21696 kubeadm.go:310] [certs] Generating "sa" key and public key
I0317 23:59:07.893074   21696 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0317 23:59:07.893074   21696 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I0317 23:59:07.893074   21696 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0317 23:59:07.893074   21696 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0317 23:59:07.893074   21696 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0317 23:59:07.893074   21696 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0317 23:59:07.893074   21696 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0317 23:59:07.893615   21696 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0317 23:59:07.894130   21696 out.go:235]     ▪ Booting up control plane ...
I0317 23:59:07.894743   21696 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0317 23:59:07.894743   21696 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0317 23:59:07.894743   21696 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0317 23:59:07.894743   21696 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0317 23:59:07.894743   21696 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0317 23:59:07.894743   21696 kubeadm.go:310] [kubelet-start] Starting the kubelet
I0317 23:59:07.894743   21696 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0317 23:59:07.895257   21696 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I0317 23:59:07.895257   21696 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 501.384569ms
I0317 23:59:07.895257   21696 kubeadm.go:310] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I0317 23:59:07.895257   21696 kubeadm.go:310] [api-check] The API server is healthy after 4.00137936s
I0317 23:59:07.895257   21696 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0317 23:59:07.895783   21696 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0317 23:59:07.895783   21696 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I0317 23:59:07.895783   21696 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0317 23:59:07.895783   21696 kubeadm.go:310] [bootstrap-token] Using token: v3thvq.rhou9cw9h8bobme3
I0317 23:59:07.896345   21696 out.go:235]     ▪ Configuring RBAC rules ...
I0317 23:59:07.896864   21696 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0317 23:59:07.896864   21696 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0317 23:59:07.896864   21696 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0317 23:59:07.897381   21696 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0317 23:59:07.897381   21696 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0317 23:59:07.897381   21696 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0317 23:59:07.897381   21696 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0317 23:59:07.897898   21696 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I0317 23:59:07.897898   21696 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I0317 23:59:07.897898   21696 kubeadm.go:310] 
I0317 23:59:07.897898   21696 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I0317 23:59:07.897898   21696 kubeadm.go:310] 
I0317 23:59:07.897898   21696 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I0317 23:59:07.897898   21696 kubeadm.go:310] 
I0317 23:59:07.897898   21696 kubeadm.go:310]   mkdir -p $HOME/.kube
I0317 23:59:07.897898   21696 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0317 23:59:07.897898   21696 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0317 23:59:07.897898   21696 kubeadm.go:310] 
I0317 23:59:07.897898   21696 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I0317 23:59:07.897898   21696 kubeadm.go:310] 
I0317 23:59:07.897898   21696 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0317 23:59:07.897898   21696 kubeadm.go:310] 
I0317 23:59:07.898411   21696 kubeadm.go:310] You should now deploy a pod network to the cluster.
I0317 23:59:07.898411   21696 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0317 23:59:07.898411   21696 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0317 23:59:07.898411   21696 kubeadm.go:310] 
I0317 23:59:07.898411   21696 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I0317 23:59:07.898411   21696 kubeadm.go:310] and service account keys on each node and then running the following as root:
I0317 23:59:07.898411   21696 kubeadm.go:310] 
I0317 23:59:07.898411   21696 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token v3thvq.rhou9cw9h8bobme3 \
I0317 23:59:07.898924   21696 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:d38d9d3c69f152b5c4d7ab6866c0a7c46869fa69c4f61566cf8cea125c2f978f \
I0317 23:59:07.898924   21696 kubeadm.go:310] 	--control-plane 
I0317 23:59:07.898924   21696 kubeadm.go:310] 
I0317 23:59:07.898924   21696 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I0317 23:59:07.898924   21696 kubeadm.go:310] 
I0317 23:59:07.898924   21696 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token v3thvq.rhou9cw9h8bobme3 \
I0317 23:59:07.898924   21696 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:d38d9d3c69f152b5c4d7ab6866c0a7c46869fa69c4f61566cf8cea125c2f978f 
I0317 23:59:07.898924   21696 cni.go:84] Creating CNI manager for ""
I0317 23:59:07.898924   21696 cni.go:158] "hyperv" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0317 23:59:07.899954   21696 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I0317 23:59:07.902014   21696 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0317 23:59:07.910875   21696 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0317 23:59:07.922153   21696 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0317 23:59:07.924811   21696 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_03_17T23_59_07_0700 minikube.k8s.io/version=v1.35.0 minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0317 23:59:07.924811   21696 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.32.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0317 23:59:07.940897   21696 ops.go:34] apiserver oom_adj: -16
I0317 23:59:08.022655   21696 kubeadm.go:1113] duration metric: took 99.9934ms to wait for elevateKubeSystemPrivileges
I0317 23:59:08.023167   21696 kubeadm.go:394] duration metric: took 8.3139087s to StartCluster
I0317 23:59:08.023701   21696 settings.go:142] acquiring lock: {Name:mka88642409b628b3541b4956f30d10c738cfc12 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0317 23:59:08.023701   21696 settings.go:150] Updating kubeconfig:  C:\Users\akank\.kube\config
I0317 23:59:08.026868   21696 lock.go:35] WriteFile acquiring C:\Users\akank\.kube\config: {Name:mk4136e5f53eedeb917065230b8ba6b3857ccf9b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0317 23:59:08.027961   21696 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.181.188 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0317 23:59:08.028512   21696 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0317 23:59:08.028512   21696 out.go:177] 🔎  Verifying Kubernetes components...
I0317 23:59:08.030090   21696 config.go:182] Loaded profile config "minikube": Driver=hyperv, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0317 23:59:08.029573   21696 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0317 23:59:08.031165   21696 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0317 23:59:08.031165   21696 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0317 23:59:08.031165   21696 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0317 23:59:08.031165   21696 addons.go:238] Setting addon storage-provisioner=true in "minikube"
I0317 23:59:08.031698   21696 host.go:66] Checking if "minikube" exists ...
I0317 23:59:08.031698   21696 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0317 23:59:08.033303   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0317 23:59:08.033852   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0317 23:59:08.239602   21696 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.176.1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0317 23:59:08.258043   21696 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0317 23:59:08.517757   21696 start.go:971] {"host.minikube.internal": 192.168.176.1} host record injected into CoreDNS's ConfigMap
I0317 23:59:08.540815   21696 api_server.go:52] waiting for apiserver process to appear ...
I0317 23:59:08.542899   21696 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0317 23:59:08.562121   21696 api_server.go:72] duration metric: took 534.1594ms to wait for apiserver process to appear ...
I0317 23:59:08.562121   21696 api_server.go:88] waiting for apiserver healthz status ...
I0317 23:59:08.562121   21696 api_server.go:253] Checking apiserver healthz at https://192.168.181.188:8443/healthz ...
I0317 23:59:08.585819   21696 api_server.go:279] https://192.168.181.188:8443/healthz returned 200:
ok
I0317 23:59:08.598278   21696 api_server.go:141] control plane version: v1.32.0
I0317 23:59:08.598278   21696 api_server.go:131] duration metric: took 36.1572ms to wait for apiserver health ...
I0317 23:59:08.598859   21696 system_pods.go:43] waiting for kube-system pods to appear ...
I0317 23:59:08.652709   21696 system_pods.go:59] 4 kube-system pods found
I0317 23:59:08.653437   21696 system_pods.go:61] "etcd-minikube" [2061c545-91b3-4490-8b93-d8e9abf24559] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0317 23:59:08.653437   21696 system_pods.go:61] "kube-apiserver-minikube" [d4a7be84-855b-4834-9231-2f3aa606b566] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0317 23:59:08.653437   21696 system_pods.go:61] "kube-controller-manager-minikube" [efbd9d43-8019-446c-8e7d-035be0755578] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0317 23:59:08.653437   21696 system_pods.go:61] "kube-scheduler-minikube" [88bb6fe6-3526-4ce1-9cfb-08173ef801b7] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0317 23:59:08.653437   21696 system_pods.go:74] duration metric: took 54.5775ms to wait for pod list to return data ...
I0317 23:59:08.653437   21696 kubeadm.go:582] duration metric: took 625.4754ms to wait for: map[apiserver:true system_pods:true]
I0317 23:59:08.653437   21696 node_conditions.go:102] verifying NodePressure condition ...
I0317 23:59:08.663947   21696 node_conditions.go:122] node storage ephemeral capacity is 17734596Ki
I0317 23:59:08.664452   21696 node_conditions.go:123] node cpu capacity is 2
I0317 23:59:08.668928   21696 node_conditions.go:105] duration metric: took 15.4918ms to run NodePressure ...
I0317 23:59:08.668928   21696 start.go:241] waiting for startup goroutines ...
I0317 23:59:08.789979   21696 main.go:141] libmachine: [stdout =====>] : Running

I0317 23:59:08.789979   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:59:08.789979   21696 main.go:141] libmachine: [stdout =====>] : Running

I0317 23:59:08.789979   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:59:08.791519   21696 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0317 23:59:08.792054   21696 addons.go:238] Setting addon default-storageclass=true in "minikube"
I0317 23:59:08.792054   21696 host.go:66] Checking if "minikube" exists ...
I0317 23:59:08.792054   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0317 23:59:08.794001   21696 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0317 23:59:08.794001   21696 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0317 23:59:08.794001   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0317 23:59:09.042675   21696 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0317 23:59:09.412839   21696 main.go:141] libmachine: [stdout =====>] : Running

I0317 23:59:09.412839   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:59:09.412839   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I0317 23:59:09.418403   21696 main.go:141] libmachine: [stdout =====>] : Running

I0317 23:59:09.418403   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:59:09.418403   21696 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0317 23:59:09.418403   21696 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0317 23:59:09.418403   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive ( Hyper-V\Get-VM minikube ).state
I0317 23:59:10.004887   21696 main.go:141] libmachine: [stdout =====>] : Running

I0317 23:59:10.004887   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:59:10.004887   21696 main.go:141] libmachine: [executing ==>] : C:\WINDOWS\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive (( Hyper-V\Get-VM minikube ).networkadapters[0]).ipaddresses[0]
I0317 23:59:10.252350   21696 main.go:141] libmachine: [stdout =====>] : 192.168.181.188

I0317 23:59:10.252350   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:59:10.252350   21696 sshutil.go:53] new ssh client: &{IP:192.168.181.188 Port:22 SSHKeyPath:C:\Users\akank\.minikube\machines\minikube\id_rsa Username:docker}
I0317 23:59:10.336182   21696 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0317 23:59:10.806567   21696 main.go:141] libmachine: [stdout =====>] : 192.168.181.188

I0317 23:59:10.806567   21696 main.go:141] libmachine: [stderr =====>] : 
I0317 23:59:10.806567   21696 sshutil.go:53] new ssh client: &{IP:192.168.181.188 Port:22 SSHKeyPath:C:\Users\akank\.minikube\machines\minikube\id_rsa Username:docker}
I0317 23:59:10.885291   21696 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0317 23:59:10.964696   21696 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass
I0317 23:59:10.965232   21696 addons.go:514] duration metric: took 2.9367201s for enable addons: enabled=[storage-provisioner default-storageclass]
I0317 23:59:10.965232   21696 start.go:246] waiting for cluster config update ...
I0317 23:59:10.965232   21696 start.go:255] writing updated cluster config ...
I0317 23:59:10.969945   21696 ssh_runner.go:195] Run: rm -f paused
I0317 23:59:10.995805   21696 out.go:177] 💡  kubectl not found. If you need it, try: 'minikube kubectl -- get pods -A'
I0317 23:59:10.996999   21696 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Mar 17 18:46:00 minikube dockerd[1417]: time="2025-03-17T18:46:00.737662000Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Mar 17 18:46:00 minikube dockerd[1417]: time="2025-03-17T18:46:00.737671590Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Mar 17 18:46:00 minikube dockerd[1417]: time="2025-03-17T18:46:00.737748164Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Mar 17 18:46:00 minikube dockerd[1411]: time="2025-03-17T18:46:00.806531747Z" level=info msg="ignoring event" container=a4feb545230a75e453c9c4a99dc7da8b01540c707f8de1b9d03e80e02e0b8c36 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 17 18:46:00 minikube dockerd[1417]: time="2025-03-17T18:46:00.806534922Z" level=info msg="shim disconnected" id=a4feb545230a75e453c9c4a99dc7da8b01540c707f8de1b9d03e80e02e0b8c36 namespace=moby
Mar 17 18:46:00 minikube dockerd[1417]: time="2025-03-17T18:46:00.806575247Z" level=warning msg="cleaning up after shim disconnected" id=a4feb545230a75e453c9c4a99dc7da8b01540c707f8de1b9d03e80e02e0b8c36 namespace=moby
Mar 17 18:46:00 minikube dockerd[1417]: time="2025-03-17T18:46:00.806580419Z" level=info msg="cleaning up dead shim" namespace=moby
Mar 17 18:46:00 minikube cri-dockerd[1310]: time="2025-03-17T18:46:00Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f: Status: Image is up to date for registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f"
Mar 17 18:46:00 minikube dockerd[1417]: time="2025-03-17T18:46:00.963697583Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Mar 17 18:46:00 minikube dockerd[1417]: time="2025-03-17T18:46:00.963776846Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Mar 17 18:46:00 minikube dockerd[1417]: time="2025-03-17T18:46:00.963788737Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Mar 17 18:46:00 minikube dockerd[1417]: time="2025-03-17T18:46:00.963860991Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Mar 17 18:46:01 minikube dockerd[1411]: time="2025-03-17T18:46:01.023557777Z" level=info msg="ignoring event" container=7bd4ce47985ff3ce1c52786c8e870b50519e793c788f0862482e3c38937ab8bd module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 17 18:46:01 minikube dockerd[1417]: time="2025-03-17T18:46:01.024120881Z" level=info msg="shim disconnected" id=7bd4ce47985ff3ce1c52786c8e870b50519e793c788f0862482e3c38937ab8bd namespace=moby
Mar 17 18:46:01 minikube dockerd[1417]: time="2025-03-17T18:46:01.024279476Z" level=warning msg="cleaning up after shim disconnected" id=7bd4ce47985ff3ce1c52786c8e870b50519e793c788f0862482e3c38937ab8bd namespace=moby
Mar 17 18:46:01 minikube dockerd[1417]: time="2025-03-17T18:46:01.024389575Z" level=info msg="cleaning up dead shim" namespace=moby
Mar 17 18:46:02 minikube dockerd[1411]: time="2025-03-17T18:46:02.032475157Z" level=warning msg="reference for unknown type: " digest="sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c" remote="docker.io/kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c" spanID=e6e3db9eb8bf6355 traceID=70f9106cf36e7e0cec93a41bd382dc6f
Mar 17 18:46:02 minikube dockerd[1411]: time="2025-03-17T18:46:02.583488810Z" level=info msg="ignoring event" container=5f7a12d4d858c183d8ade3c2de10af9683139bc106e4c5325d98755cbaa65abb module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 17 18:46:02 minikube dockerd[1417]: time="2025-03-17T18:46:02.584141829Z" level=info msg="shim disconnected" id=5f7a12d4d858c183d8ade3c2de10af9683139bc106e4c5325d98755cbaa65abb namespace=moby
Mar 17 18:46:02 minikube dockerd[1417]: time="2025-03-17T18:46:02.584497640Z" level=warning msg="cleaning up after shim disconnected" id=5f7a12d4d858c183d8ade3c2de10af9683139bc106e4c5325d98755cbaa65abb namespace=moby
Mar 17 18:46:02 minikube dockerd[1417]: time="2025-03-17T18:46:02.584530961Z" level=info msg="cleaning up dead shim" namespace=moby
Mar 17 18:46:02 minikube dockerd[1411]: time="2025-03-17T18:46:02.598049239Z" level=info msg="ignoring event" container=a0cfa6b09eeeeadfa9fba56c4db1df70a87866cbf8b7b34873b2de3196a61108 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 17 18:46:02 minikube dockerd[1417]: time="2025-03-17T18:46:02.598869439Z" level=info msg="shim disconnected" id=a0cfa6b09eeeeadfa9fba56c4db1df70a87866cbf8b7b34873b2de3196a61108 namespace=moby
Mar 17 18:46:02 minikube dockerd[1417]: time="2025-03-17T18:46:02.598935368Z" level=warning msg="cleaning up after shim disconnected" id=a0cfa6b09eeeeadfa9fba56c4db1df70a87866cbf8b7b34873b2de3196a61108 namespace=moby
Mar 17 18:46:02 minikube dockerd[1417]: time="2025-03-17T18:46:02.598943929Z" level=info msg="cleaning up dead shim" namespace=moby
Mar 17 18:46:14 minikube cri-dockerd[1310]: time="2025-03-17T18:46:14Z" level=info msg="Pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: 5866d2c04d96: Pulling fs layer "
Mar 17 18:46:24 minikube cri-dockerd[1310]: time="2025-03-17T18:46:24Z" level=info msg="Pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: 5866d2c04d96: Pulling fs layer "
Mar 17 18:46:29 minikube dockerd[1417]: time="2025-03-17T18:46:29.838951360Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Mar 17 18:46:29 minikube dockerd[1417]: time="2025-03-17T18:46:29.839023161Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Mar 17 18:46:29 minikube dockerd[1417]: time="2025-03-17T18:46:29.839038466Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Mar 17 18:46:29 minikube dockerd[1417]: time="2025-03-17T18:46:29.839106021Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Mar 17 18:46:29 minikube cri-dockerd[1310]: time="2025-03-17T18:46:29Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/dba41945900e511a5135554a00b8873dbe22dcc305ea61f6a045d040f2e62063/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 17 18:46:34 minikube cri-dockerd[1310]: time="2025-03-17T18:46:34Z" level=info msg="Pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: 5866d2c04d96: Pulling fs layer "
Mar 17 18:46:44 minikube cri-dockerd[1310]: time="2025-03-17T18:46:44Z" level=info msg="Pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: 5866d2c04d96: Pulling fs layer "
Mar 17 18:46:54 minikube cri-dockerd[1310]: time="2025-03-17T18:46:54Z" level=info msg="Pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: 5866d2c04d96: Pulling fs layer "
Mar 17 18:47:04 minikube cri-dockerd[1310]: time="2025-03-17T18:47:04Z" level=info msg="Pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: 5866d2c04d96: Pulling fs layer "
Mar 17 18:47:12 minikube dockerd[1411]: time="2025-03-17T18:47:12.693073019Z" level=info msg="Download failed, retrying (1/5): dial tcp 104.16.98.215:443: i/o timeout"
Mar 17 18:47:12 minikube dockerd[1411]: time="2025-03-17T18:47:12.726741364Z" level=info msg="Download failed, retrying (1/5): dial tcp 104.16.98.215:443: i/o timeout"
Mar 17 18:47:14 minikube cri-dockerd[1310]: time="2025-03-17T18:47:14Z" level=info msg="Pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: 978be80e3ee3: Retrying in 4 seconds "
Mar 17 18:47:24 minikube cri-dockerd[1310]: time="2025-03-17T18:47:24Z" level=info msg="Pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: 978be80e3ee3: Retrying in 1 second "
Mar 17 18:47:34 minikube cri-dockerd[1310]: time="2025-03-17T18:47:34Z" level=info msg="Pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: 978be80e3ee3: Retrying in 1 second "
Mar 17 18:47:44 minikube cri-dockerd[1310]: time="2025-03-17T18:47:44Z" level=info msg="Pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: 978be80e3ee3: Retrying in 1 second "
Mar 17 18:47:54 minikube cri-dockerd[1310]: time="2025-03-17T18:47:54Z" level=info msg="Pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: 978be80e3ee3: Retrying in 1 second "
Mar 17 18:48:04 minikube cri-dockerd[1310]: time="2025-03-17T18:48:04Z" level=info msg="Pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: 978be80e3ee3: Retrying in 1 second "
Mar 17 18:48:14 minikube cri-dockerd[1310]: time="2025-03-17T18:48:14Z" level=info msg="Pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: 978be80e3ee3: Retrying in 1 second "
Mar 17 18:48:19 minikube dockerd[1411]: time="2025-03-17T18:48:19.982345926Z" level=info msg="Download failed, retrying (2/5): dial tcp 104.16.98.215:443: i/o timeout"
Mar 17 18:48:19 minikube dockerd[1411]: time="2025-03-17T18:48:19.982346053Z" level=info msg="Download failed, retrying (2/5): dial tcp 104.16.98.215:443: i/o timeout"
Mar 17 18:48:24 minikube cri-dockerd[1310]: time="2025-03-17T18:48:24Z" level=info msg="Pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: 978be80e3ee3: Retrying in 6 seconds "
Mar 17 18:48:34 minikube cri-dockerd[1310]: time="2025-03-17T18:48:34Z" level=info msg="Pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: 5866d2c04d96: Retrying in 1 second "
Mar 17 18:48:44 minikube cri-dockerd[1310]: time="2025-03-17T18:48:44Z" level=info msg="Pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: 5866d2c04d96: Retrying in 1 second "
Mar 17 18:48:49 minikube dockerd[1411]: time="2025-03-17T18:48:49.742068140Z" level=error msg="Not continuing with pull after error: error pulling image configuration: download failed after attempts=6: dial tcp 104.16.98.215:443: i/o timeout" spanID=e6e3db9eb8bf6355 traceID=70f9106cf36e7e0cec93a41bd382dc6f
Mar 17 18:48:49 minikube cri-dockerd[1310]: time="2025-03-17T18:48:49Z" level=info msg="Stop pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: 5866d2c04d96: Retrying in 1 second "
Mar 17 18:48:50 minikube dockerd[1411]: time="2025-03-17T18:48:50.056971053Z" level=warning msg="reference for unknown type: " digest="sha256:ffcb2bf004d6aa0a17d90e0247cf94f2865c8901dcab4427034c341951c239f9" remote="registry.k8s.io/metrics-server/metrics-server@sha256:ffcb2bf004d6aa0a17d90e0247cf94f2865c8901dcab4427034c341951c239f9" spanID=e84ed1ee26a1cb67 traceID=59e399b78802d63b15a311cacaedba6c
Mar 17 18:49:00 minikube cri-dockerd[1310]: time="2025-03-17T18:49:00Z" level=info msg="Pulling image registry.k8s.io/metrics-server/metrics-server:v0.7.2@sha256:ffcb2bf004d6aa0a17d90e0247cf94f2865c8901dcab4427034c341951c239f9: c530a5b08991: Downloading [=====>                                             ]  1.913MB/18.7MB"
Mar 17 18:49:10 minikube cri-dockerd[1310]: time="2025-03-17T18:49:10Z" level=info msg="Pulling image registry.k8s.io/metrics-server/metrics-server:v0.7.2@sha256:ffcb2bf004d6aa0a17d90e0247cf94f2865c8901dcab4427034c341951c239f9: c530a5b08991: Downloading [==========================>                        ]  9.764MB/18.7MB"
Mar 17 18:49:18 minikube cri-dockerd[1310]: time="2025-03-17T18:49:18Z" level=info msg="Stop pulling image registry.k8s.io/metrics-server/metrics-server:v0.7.2@sha256:ffcb2bf004d6aa0a17d90e0247cf94f2865c8901dcab4427034c341951c239f9: Status: Downloaded newer image for registry.k8s.io/metrics-server/metrics-server@sha256:ffcb2bf004d6aa0a17d90e0247cf94f2865c8901dcab4427034c341951c239f9"
Mar 17 18:49:18 minikube dockerd[1417]: time="2025-03-17T18:49:18.769452195Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Mar 17 18:49:18 minikube dockerd[1417]: time="2025-03-17T18:49:18.769514321Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Mar 17 18:49:18 minikube dockerd[1417]: time="2025-03-17T18:49:18.769726796Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Mar 17 18:49:18 minikube dockerd[1417]: time="2025-03-17T18:49:18.770245275Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1


==> container status <==
CONTAINER           IMAGE                                                                                                                        CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
1538f0ab7cc88       registry.k8s.io/metrics-server/metrics-server@sha256:ffcb2bf004d6aa0a17d90e0247cf94f2865c8901dcab4427034c341951c239f9        10 seconds ago      Running             metrics-server            0                   234b44babbd8a       metrics-server-7fbb699795-ltl6n
7bd4ce47985ff       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f   3 minutes ago       Exited              patch                     0                   a0cfa6b09eeee       ingress-nginx-admission-patch-kf7pl
a4feb545230a7       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f   3 minutes ago       Exited              create                    0                   5f7a12d4d858c       ingress-nginx-admission-create-qn7kk
f388f86d05203       gcr.io/k8s-minikube/minikube-ingress-dns@sha256:4211a1de532376c881851542238121b26792225faa36a7b02dccad88fd05797c             4 minutes ago       Running             minikube-ingress-dns      0                   b64707d571f9b       kube-ingress-dns-minikube
0bf9a56263932       6e38f40d628db                                                                                                                19 minutes ago      Running             storage-provisioner       1                   9102bf1803d3d       storage-provisioner
976b6321cee22       040f9f8aac8cd                                                                                                                20 minutes ago      Running             kube-proxy                0                   53b911a4eb257       kube-proxy-ppzwv
11d59a63cfcf0       c69fa2e9cbf5f                                                                                                                20 minutes ago      Running             coredns                   0                   1fc09509f3c92       coredns-668d6bf9bc-ljvg5
851579b7d45a8       6e38f40d628db                                                                                                                20 minutes ago      Exited              storage-provisioner       0                   9102bf1803d3d       storage-provisioner
8344361f4d456       a389e107f4ff1                                                                                                                20 minutes ago      Running             kube-scheduler            0                   7764933800cdb       kube-scheduler-minikube
432ca75cd7864       8cab3d2a8bd0f                                                                                                                20 minutes ago      Running             kube-controller-manager   0                   9553cbb893255       kube-controller-manager-minikube
d2cc9ac400b74       a9e7e6b294baf                                                                                                                20 minutes ago      Running             etcd                      0                   ca7f47f8dd269       etcd-minikube
5f4824f4a0c1c       c2e17b8d0f4a3                                                                                                                20 minutes ago      Running             kube-apiserver            0                   7984f8784688e       kube-apiserver-minikube


==> coredns [11d59a63cfcf] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = e2bd9727e0512d2ccc05333f6ceed26da5586745f340d41f867ca605c244a7ed3a49c84eb8c030a83feedfe13eae4505c960d989fabf542f0ce8e03ba15206db
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] 127.0.0.1:60138 - 17298 "HINFO IN 4376521428137665615.6565459225707697017. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.115674975s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[818830689]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (17-Mar-2025 18:29:13.104) (total time: 30000ms):
Trace[818830689]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30000ms (18:29:43.105)
Trace[818830689]: [30.000477325s] [30.000477325s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1099565492]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (17-Mar-2025 18:29:13.105) (total time: 30000ms):
Trace[1099565492]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30000ms (18:29:43.105)
Trace[1099565492]: [30.00015479s] [30.00015479s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1902494556]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (17-Mar-2025 18:29:13.105) (total time: 30000ms):
Trace[1902494556]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30000ms (18:29:43.105)
Trace[1902494556]: [30.000158308s] [30.000158308s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_03_17T23_59_07_0700
                    minikube.k8s.io/version=v1.35.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 17 Mar 2025 18:29:04 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Mon, 17 Mar 2025 18:49:21 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Mon, 17 Mar 2025 18:46:16 +0000   Mon, 17 Mar 2025 18:29:03 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Mon, 17 Mar 2025 18:46:16 +0000   Mon, 17 Mar 2025 18:29:03 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Mon, 17 Mar 2025 18:46:16 +0000   Mon, 17 Mar 2025 18:29:03 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Mon, 17 Mar 2025 18:46:16 +0000   Mon, 17 Mar 2025 18:29:11 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.181.188
  Hostname:    minikube
Capacity:
  cpu:                2
  ephemeral-storage:  17734596Ki
  hugepages-2Mi:      0
  memory:             3878440Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  17734596Ki
  hugepages-2Mi:      0
  memory:             3878440Ki
  pods:               110
System Info:
  Machine ID:                 55cf14f8819342df8b9762f4e9fa5313
  System UUID:                53c9be86-4916-0a48-8f1e-1bbc2b5d120b
  Boot ID:                    f468b9b5-e3b9-4872-bb64-ea261d36657e
  Kernel Version:             5.10.207
  OS Image:                   Buildroot 2023.02.9
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.4.0
  Kubelet Version:            v1.32.0
  Kube-Proxy Version:         v1.32.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (17 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  default                     cpu-utilization-job-29037280-4854z            0 (0%)        0 (0%)      0 (0%)           0 (0%)         9m28s
  default                     cpu-utilization-job-29037285-8vlzq            0 (0%)        0 (0%)      0 (0%)           0 (0%)         4m28s
  default                     ml-model-deployment-644fd64744-wfrg4          0 (0%)        0 (0%)      0 (0%)           0 (0%)         12m
  ingress-nginx               ingress-nginx-controller-56d7c84fd4-ttg2j     100m (5%)     0 (0%)      90Mi (2%)        0 (0%)         9m11s
  kube-system                 coredns-668d6bf9bc-ljvg5                      100m (5%)     0 (0%)      70Mi (1%)        170Mi (4%)     20m
  kube-system                 etcd-minikube                                 100m (5%)     0 (0%)      100Mi (2%)       0 (0%)         20m
  kube-system                 kube-apiserver-minikube                       250m (12%)    0 (0%)      0 (0%)           0 (0%)         20m
  kube-system                 kube-controller-manager-minikube              200m (10%)    0 (0%)      0 (0%)           0 (0%)         20m
  kube-system                 kube-ingress-dns-minikube                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         9m17s
  kube-system                 kube-proxy-ppzwv                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         20m
  kube-system                 kube-scheduler-minikube                       100m (5%)     0 (0%)      0 (0%)           0 (0%)         20m
  kube-system                 metrics-server-7fbb699795-ltl6n               100m (5%)     0 (0%)      200Mi (5%)       0 (0%)         7m47s
  kube-system                 storage-provisioner                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         20m
  kubernetes-dashboard        dashboard-metrics-scraper-5bd45c9dd6-7r8bk    0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m57s
  kubernetes-dashboard        dashboard-metrics-scraper-5d59dccf9b-9stv2    0 (0%)        0 (0%)      0 (0%)           0 (0%)         19m
  kubernetes-dashboard        kubernetes-dashboard-7779f9b69b-rgj2t         0 (0%)        0 (0%)      0 (0%)           0 (0%)         19m
  kubernetes-dashboard        kubernetes-dashboard-79cbcf9fb6-s44ht         0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m57s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                950m (47%)   0 (0%)
  memory             460Mi (12%)  170Mi (4%)
  ephemeral-storage  0 (0%)       0 (0%)
  hugepages-2Mi      0 (0%)       0 (0%)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 20m                kube-proxy       
  Normal  NodeHasSufficientMemory  20m (x8 over 20m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    20m (x8 over 20m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     20m (x7 over 20m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  Starting                 20m                kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  20m                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  20m                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    20m                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     20m                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           20m                node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  NodeReady                20m                kubelet          Node minikube status is now: NodeReady


==> dmesg <==
[Mar17 18:28] You have booted with nomodeset. This means your GPU drivers are DISABLED
[  +0.000000] Any video related functionality will be severely degraded, and you may not even be able to suspend the system properly
[  +0.000001] Unless you actually understand what nomodeset does, you should reboot without enabling it
[  +0.052139] Spectre V2 : WARNING: Unprivileged eBPF is enabled with eIBRS on, data leaks possible via Spectre v2 BHB attacks!
[  +0.029210] acpi PNP0A03:00: fail to add MMCONFIG information, can't access extended PCI configuration space under this bridge.
[  +0.009655] * Found PM-Timer Bug on the chipset. Due to workarounds for a bug,
              * this clock source is slow. Consider trying other clock sources
[  +4.123315] platform regulatory.0: Direct firmware load for regulatory.db failed with error -2
[  +0.716017] psmouse serio1: trackpoint: failed to get extended button data, assuming 3 buttons
[  +0.495405] systemd-fstab-generator[115]: Ignoring "noauto" option for root device
[  +2.448974] NFSD: Using /var/lib/nfs/v4recovery as the NFSv4 state recovery directory
[  +0.000008] NFSD: unable to find recovery directory /var/lib/nfs/v4recovery
[  +0.000001] NFSD: Unable to initialize client recovery tracking! (-2)
[ +14.522962] systemd-fstab-generator[634]: Ignoring "noauto" option for root device
[  +0.086962] systemd-fstab-generator[646]: Ignoring "noauto" option for root device
[  +7.920217] systemd-fstab-generator[976]: Ignoring "noauto" option for root device
[  +0.049994] kauditd_printk_skb: 69 callbacks suppressed
[  +0.194949] systemd-fstab-generator[1013]: Ignoring "noauto" option for root device
[  +0.097447] systemd-fstab-generator[1025]: Ignoring "noauto" option for root device
[  +0.103030] systemd-fstab-generator[1039]: Ignoring "noauto" option for root device
[  +2.289544] systemd-fstab-generator[1263]: Ignoring "noauto" option for root device
[  +0.093856] systemd-fstab-generator[1275]: Ignoring "noauto" option for root device
[  +0.084028] systemd-fstab-generator[1287]: Ignoring "noauto" option for root device
[  +0.103462] systemd-fstab-generator[1302]: Ignoring "noauto" option for root device
[  +3.794146] systemd-fstab-generator[1403]: Ignoring "noauto" option for root device
[  +0.061498] kauditd_printk_skb: 206 callbacks suppressed
[  +3.055037] systemd-fstab-generator[1662]: Ignoring "noauto" option for root device
[Mar17 18:29] systemd-fstab-generator[1779]: Ignoring "noauto" option for root device
[  +0.050877] kauditd_printk_skb: 74 callbacks suppressed
[  +4.953282] systemd-fstab-generator[2206]: Ignoring "noauto" option for root device
[  +0.055445] kauditd_printk_skb: 62 callbacks suppressed
[  +1.118388] systemd-fstab-generator[2265]: Ignoring "noauto" option for root device
[  +4.734455] kauditd_printk_skb: 34 callbacks suppressed
[ +27.709576] kauditd_printk_skb: 55 callbacks suppressed
[  +5.899558] kauditd_printk_skb: 26 callbacks suppressed
[Mar17 18:37] kauditd_printk_skb: 6 callbacks suppressed
[Mar17 18:40] kauditd_printk_skb: 2 callbacks suppressed
[Mar17 18:41] kauditd_printk_skb: 20 callbacks suppressed
[Mar17 18:43] kauditd_printk_skb: 6 callbacks suppressed
[Mar17 18:45] kauditd_printk_skb: 2 callbacks suppressed
[ +32.911603] kauditd_printk_skb: 25 callbacks suppressed
[Mar17 18:46] kauditd_printk_skb: 16 callbacks suppressed
[Mar17 18:49] kauditd_printk_skb: 18 callbacks suppressed


==> etcd [d2cc9ac400b7] <==
{"level":"warn","ts":"2025-03-17T18:29:03.105599Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-03-17T18:29:03.105675Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.181.188:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.181.188:2380","--initial-cluster=minikube=https://192.168.181.188:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.181.188:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.181.188:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"warn","ts":"2025-03-17T18:29:03.105753Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-03-17T18:29:03.105761Z","caller":"embed/etcd.go:128","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.181.188:2380"]}
{"level":"info","ts":"2025-03-17T18:29:03.105782Z","caller":"embed/etcd.go:497","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-03-17T18:29:03.106142Z","caller":"embed/etcd.go:136","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.181.188:2379"]}
{"level":"info","ts":"2025-03-17T18:29:03.106212Z","caller":"embed/etcd.go:311","msg":"starting an etcd server","etcd-version":"3.5.16","git-sha":"f20bbad","go-version":"go1.22.7","go-os":"linux","go-arch":"amd64","max-cpu-set":2,"max-cpu-available":2,"member-initialized":false,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.181.188:2380"],"listen-peer-urls":["https://192.168.181.188:2380"],"advertise-client-urls":["https://192.168.181.188:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.181.188:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"minikube=https://192.168.181.188:2380","initial-cluster-state":"new","initial-cluster-token":"etcd-cluster","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2025-03-17T18:29:03.114603Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"8.183847ms"}
{"level":"info","ts":"2025-03-17T18:29:03.158272Z","caller":"etcdserver/raft.go:505","msg":"starting local member","local-member-id":"1881b067206b7198","cluster-id":"8feaf495e192a38d"}
{"level":"info","ts":"2025-03-17T18:29:03.158440Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"1881b067206b7198 switched to configuration voters=()"}
{"level":"info","ts":"2025-03-17T18:29:03.158515Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"1881b067206b7198 became follower at term 0"}
{"level":"info","ts":"2025-03-17T18:29:03.158895Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft 1881b067206b7198 [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]"}
{"level":"info","ts":"2025-03-17T18:29:03.158949Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"1881b067206b7198 became follower at term 1"}
{"level":"info","ts":"2025-03-17T18:29:03.161314Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"1881b067206b7198 switched to configuration voters=(1765886485877977496)"}
{"level":"warn","ts":"2025-03-17T18:29:03.172933Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2025-03-17T18:29:03.181606Z","caller":"mvcc/kvstore.go:423","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2025-03-17T18:29:03.190319Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-03-17T18:29:03.205687Z","caller":"etcdserver/server.go:873","msg":"starting etcd server","local-member-id":"1881b067206b7198","local-server-version":"3.5.16","cluster-version":"to_be_decided"}
{"level":"info","ts":"2025-03-17T18:29:03.206311Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-03-17T18:29:03.207546Z","caller":"etcdserver/server.go:757","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"1881b067206b7198","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2025-03-17T18:29:03.207821Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-03-17T18:29:03.207912Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-03-17T18:29:03.207967Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-03-17T18:29:03.229552Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"1881b067206b7198 switched to configuration voters=(1765886485877977496)"}
{"level":"info","ts":"2025-03-17T18:29:03.230715Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"8feaf495e192a38d","local-member-id":"1881b067206b7198","added-peer-id":"1881b067206b7198","added-peer-peer-urls":["https://192.168.181.188:2380"]}
{"level":"info","ts":"2025-03-17T18:29:03.231418Z","caller":"embed/etcd.go:729","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-03-17T18:29:03.231761Z","caller":"embed/etcd.go:280","msg":"now serving peer/client/metrics","local-member-id":"1881b067206b7198","initial-advertise-peer-urls":["https://192.168.181.188:2380"],"listen-peer-urls":["https://192.168.181.188:2380"],"advertise-client-urls":["https://192.168.181.188:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.181.188:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-03-17T18:29:03.232293Z","caller":"embed/etcd.go:871","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-03-17T18:29:03.232602Z","caller":"embed/etcd.go:600","msg":"serving peer traffic","address":"192.168.181.188:2380"}
{"level":"info","ts":"2025-03-17T18:29:03.232958Z","caller":"embed/etcd.go:572","msg":"cmux::serve","address":"192.168.181.188:2380"}
{"level":"info","ts":"2025-03-17T18:29:03.862279Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"1881b067206b7198 is starting a new election at term 1"}
{"level":"info","ts":"2025-03-17T18:29:03.862391Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"1881b067206b7198 became pre-candidate at term 1"}
{"level":"info","ts":"2025-03-17T18:29:03.862432Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"1881b067206b7198 received MsgPreVoteResp from 1881b067206b7198 at term 1"}
{"level":"info","ts":"2025-03-17T18:29:03.862450Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"1881b067206b7198 became candidate at term 2"}
{"level":"info","ts":"2025-03-17T18:29:03.862460Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"1881b067206b7198 received MsgVoteResp from 1881b067206b7198 at term 2"}
{"level":"info","ts":"2025-03-17T18:29:03.862469Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"1881b067206b7198 became leader at term 2"}
{"level":"info","ts":"2025-03-17T18:29:03.862476Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: 1881b067206b7198 elected leader 1881b067206b7198 at term 2"}
{"level":"info","ts":"2025-03-17T18:29:03.867471Z","caller":"etcdserver/server.go:2140","msg":"published local member to cluster through raft","local-member-id":"1881b067206b7198","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.181.188:2379]}","request-path":"/0/members/1881b067206b7198/attributes","cluster-id":"8feaf495e192a38d","publish-timeout":"7s"}
{"level":"info","ts":"2025-03-17T18:29:03.867726Z","caller":"etcdserver/server.go:2651","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2025-03-17T18:29:03.870320Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-03-17T18:29:03.867793Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-03-17T18:29:03.867805Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-03-17T18:29:03.870698Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-03-17T18:29:03.873587Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-03-17T18:29:03.874064Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-03-17T18:29:03.874390Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"8feaf495e192a38d","local-member-id":"1881b067206b7198","cluster-version":"3.5"}
{"level":"info","ts":"2025-03-17T18:29:03.874519Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-03-17T18:29:03.874571Z","caller":"etcdserver/server.go:2675","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2025-03-17T18:29:03.874960Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-03-17T18:29:03.874117Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.181.188:2379"}
{"level":"info","ts":"2025-03-17T18:39:03.925303Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":708}
{"level":"info","ts":"2025-03-17T18:39:03.929232Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":708,"took":"3.443178ms","hash":2390251123,"current-db-size-bytes":1921024,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1921024,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-03-17T18:39:03.929278Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2390251123,"revision":708,"compact-revision":-1}
{"level":"info","ts":"2025-03-17T18:44:03.929464Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":985}
{"level":"info","ts":"2025-03-17T18:44:03.931483Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":985,"took":"1.81042ms","hash":3443017318,"current-db-size-bytes":1921024,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1859584,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-03-17T18:44:03.931517Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3443017318,"revision":985,"compact-revision":708}
{"level":"info","ts":"2025-03-17T18:49:03.934687Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1359}
{"level":"info","ts":"2025-03-17T18:49:03.938080Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":1359,"took":"2.988846ms","hash":184098243,"current-db-size-bytes":2134016,"current-db-size":"2.1 MB","current-db-size-in-use-bytes":2134016,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2025-03-17T18:49:03.938168Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":184098243,"revision":1359,"compact-revision":985}


==> kernel <==
 18:49:28 up 21 min,  0 users,  load average: 0.14, 0.19, 0.17
Linux minikube 5.10.207 #1 SMP Tue Jan 14 08:15:54 UTC 2025 x86_64 GNU/Linux
PRETTY_NAME="Buildroot 2023.02.9"


==> kube-apiserver [5f4824f4a0c1] <==
	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I0317 18:41:42.516376       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0317 18:41:42.516457       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0317 18:42:42.516751       1 handler_proxy.go:99] no RequestInfo found in the context
E0317 18:42:42.516797       1 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService" logger="UnhandledError"
W0317 18:42:42.516830       1 handler_proxy.go:99] no RequestInfo found in the context
E0317 18:42:42.516878       1 controller.go:102] "Unhandled Error" err=<
	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I0317 18:42:42.518606       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0317 18:42:42.518747       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0317 18:44:04.809221       1 handler_proxy.go:99] no RequestInfo found in the context
E0317 18:44:04.809491       1 controller.go:146] "Unhandled Error" err=<
	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
W0317 18:44:05.811902       1 handler_proxy.go:99] no RequestInfo found in the context
E0317 18:44:05.811980       1 controller.go:102] "Unhandled Error" err=<
	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
W0317 18:44:05.811902       1 handler_proxy.go:99] no RequestInfo found in the context
E0317 18:44:05.811999       1 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService" logger="UnhandledError"
I0317 18:44:05.813145       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0317 18:44:05.813180       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0317 18:45:05.814228       1 handler_proxy.go:99] no RequestInfo found in the context
E0317 18:45:05.814277       1 controller.go:102] "Unhandled Error" err=<
	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
W0317 18:45:05.814399       1 handler_proxy.go:99] no RequestInfo found in the context
E0317 18:45:05.814465       1 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService" logger="UnhandledError"
I0317 18:45:05.815874       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0317 18:45:05.815890       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0317 18:47:05.816183       1 handler_proxy.go:99] no RequestInfo found in the context
E0317 18:47:05.816228       1 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService" logger="UnhandledError"
W0317 18:47:05.816188       1 handler_proxy.go:99] no RequestInfo found in the context
E0317 18:47:05.816360       1 controller.go:102] "Unhandled Error" err=<
	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I0317 18:47:05.817423       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0317 18:47:05.817463       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0317 18:49:04.814443       1 handler_proxy.go:99] no RequestInfo found in the context
E0317 18:49:04.814702       1 controller.go:146] "Unhandled Error" err=<
	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
W0317 18:49:05.817224       1 handler_proxy.go:99] no RequestInfo found in the context
W0317 18:49:05.817224       1 handler_proxy.go:99] no RequestInfo found in the context
E0317 18:49:05.817321       1 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService" logger="UnhandledError"
E0317 18:49:05.817398       1 controller.go:102] "Unhandled Error" err=<
	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I0317 18:49:05.818435       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0317 18:49:05.818441       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.


==> kube-controller-manager [432ca75cd786] <==
I0317 18:42:12.909768       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
E0317 18:42:41.421910       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0317 18:42:41.424039       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
I0317 18:43:10.739241       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ml-model-deployment-644fd64744" duration="22.787µs"
E0317 18:43:11.426992       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0317 18:43:11.429190       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
I0317 18:43:26.102155       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ml-model-deployment-644fd64744" duration="30.598µs"
E0317 18:43:41.432109       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0317 18:43:41.433993       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
E0317 18:44:11.437094       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0317 18:44:11.438104       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
I0317 18:44:26.081501       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="default/cpu-utilization-job-29037280" delay="1s"
I0317 18:44:34.093311       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="82.234µs"
I0317 18:44:38.091156       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="default/cpu-utilization-job-29037280" delay="1s"
E0317 18:44:41.441182       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0317 18:44:41.442170       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
I0317 18:44:49.094257       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="40.334µs"
I0317 18:45:00.106552       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="default/cpu-utilization-job-29037285" delay="0s"
I0317 18:45:00.115820       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="default/cpu-utilization-job-29037285" delay="1s"
I0317 18:45:00.121552       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="default/cpu-utilization-job-29037285" delay="1s"
I0317 18:45:00.122688       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="default/cpu-utilization-job-29037285" delay="1s"
I0317 18:45:00.130604       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="default/cpu-utilization-job-29037285" delay="1s"
E0317 18:45:11.445755       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0317 18:45:11.447264       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
I0317 18:45:31.589255       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-79cbcf9fb6" duration="44.423435ms"
I0317 18:45:31.604711       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-79cbcf9fb6" duration="15.280907ms"
I0317 18:45:31.604989       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-79cbcf9fb6" duration="57.413µs"
I0317 18:45:31.613381       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-79cbcf9fb6" duration="180.059µs"
I0317 18:45:31.699629       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5bd45c9dd6" duration="26.871647ms"
I0317 18:45:31.761305       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5bd45c9dd6" duration="61.623588ms"
I0317 18:45:31.761476       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5bd45c9dd6" duration="42.489µs"
E0317 18:45:41.450385       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0317 18:45:41.451296       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
I0317 18:45:46.370132       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0317 18:46:01.532908       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0317 18:46:01.539390       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0317 18:46:02.629321       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0317 18:46:02.669333       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0317 18:46:03.642862       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0317 18:46:03.663784       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0317 18:46:03.682148       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0317 18:46:03.688390       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
E0317 18:46:11.456389       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0317 18:46:11.459632       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
I0317 18:46:16.662749       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
E0317 18:46:41.461163       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0317 18:46:41.463741       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
E0317 18:47:11.466109       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0317 18:47:11.467974       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
E0317 18:47:41.470829       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0317 18:47:41.472385       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
E0317 18:48:11.475566       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0317 18:48:11.477165       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
E0317 18:48:41.479131       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0317 18:48:41.481837       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
I0317 18:49:05.094237       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="44.018µs"
E0317 18:49:11.482936       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0317 18:49:11.486352       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
I0317 18:49:17.103607       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="35.584µs"
I0317 18:49:19.581079       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-7fbb699795" duration="29.228µs"


==> kube-proxy [976b6321cee2] <==
I0317 18:29:14.349673       1 server_linux.go:66] "Using iptables proxy"
E0317 18:29:14.363518       1 proxier.go:733] "Error cleaning up nftables rules" err=<
	could not run nftables command: /dev/stdin:1:1-24: Error: Could not process rule: Operation not supported
	add table ip kube-proxy
	^^^^^^^^^^^^^^^^^^^^^^^^
 >
E0317 18:29:14.370105       1 proxier.go:733] "Error cleaning up nftables rules" err=<
	could not run nftables command: /dev/stdin:1:1-25: Error: Could not process rule: Operation not supported
	add table ip6 kube-proxy
	^^^^^^^^^^^^^^^^^^^^^^^^^
 >
I0317 18:29:14.376887       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.181.188"]
E0317 18:29:14.376951       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0317 18:29:14.390538       1 server_linux.go:147] "No iptables support for family" ipFamily="IPv6"
I0317 18:29:14.390569       1 server.go:245] "kube-proxy running in single-stack mode" ipFamily="IPv4"
I0317 18:29:14.390586       1 server_linux.go:170] "Using iptables Proxier"
I0317 18:29:14.391607       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0317 18:29:14.391762       1 server.go:497] "Version info" version="v1.32.0"
I0317 18:29:14.391823       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0317 18:29:14.392530       1 config.go:199] "Starting service config controller"
I0317 18:29:14.392561       1 shared_informer.go:313] Waiting for caches to sync for service config
I0317 18:29:14.392594       1 config.go:105] "Starting endpoint slice config controller"
I0317 18:29:14.392613       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0317 18:29:14.392904       1 config.go:329] "Starting node config controller"
I0317 18:29:14.392970       1 shared_informer.go:313] Waiting for caches to sync for node config
I0317 18:29:14.493165       1 shared_informer.go:320] Caches are synced for service config
I0317 18:29:14.493165       1 shared_informer.go:320] Caches are synced for node config
I0317 18:29:14.493177       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-scheduler [8344361f4d45] <==
I0317 18:29:03.777898       1 serving.go:386] Generated self-signed cert in-memory
W0317 18:29:04.767200       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0317 18:29:04.767477       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0317 18:29:04.767599       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0317 18:29:04.767684       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0317 18:29:04.799259       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0317 18:29:04.802245       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0317 18:29:04.808932       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0317 18:29:04.811529       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0317 18:29:04.811728       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0317 18:29:04.811862       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
W0317 18:29:04.820478       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0317 18:29:04.821354       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0317 18:29:04.821547       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0317 18:29:04.821605       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0317 18:29:04.821618       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
E0317 18:29:04.821803       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0317 18:29:04.821871       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0317 18:29:04.821906       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0317 18:29:04.820707       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0317 18:29:04.821978       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0317 18:29:04.820739       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0317 18:29:04.822047       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0317 18:29:04.820761       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0317 18:29:04.822100       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0317 18:29:04.820781       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0317 18:29:04.822153       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0317 18:29:04.820799       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0317 18:29:04.822237       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
E0317 18:29:04.822257       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0317 18:29:04.820839       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0317 18:29:04.822271       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0317 18:29:04.820856       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0317 18:29:04.822291       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0317 18:29:04.820899       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0317 18:29:04.822301       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0317 18:29:04.820934       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0317 18:29:04.822308       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0317 18:29:04.821015       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0317 18:29:04.822317       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0317 18:29:04.821039       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0317 18:29:04.822326       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E0317 18:29:04.822395       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0317 18:29:05.678702       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0317 18:29:05.678747       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0317 18:29:05.698356       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0317 18:29:05.698543       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0317 18:29:05.712902       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0317 18:29:05.712933       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0317 18:29:05.714669       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0317 18:29:05.714691       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0317 18:29:05.759143       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0317 18:29:05.759178       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0317 18:29:05.767398       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0317 18:29:05.767592       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0317 18:29:05.823999       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
E0317 18:29:05.824171       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0317 18:29:06.026069       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0317 18:29:06.026247       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
I0317 18:29:08.415888       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Mar 17 18:44:21 minikube kubelet[2213]: E0317 18:44:21.127122    2213 kuberuntime_image.go:55] "Failed to pull image" err="rpc error: code = Canceled desc = context canceled" image="docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93"
Mar 17 18:44:21 minikube kubelet[2213]: E0317 18:44:21.127349    2213 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:kubernetes-dashboard,Image:docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93,Command:[],Args:[--namespace=kubernetes-dashboard --enable-skip-login --disable-settings-authorizer],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:9090,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:tmp-volume,ReadOnly:false,MountPath:/tmp,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-p84wv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/,Port:{0 9090 },Host:,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:30,TimeoutSeconds:30,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,},ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:*1001,RunAsNonRoot:nil,ReadOnlyRootFilesystem:*true,AllowPrivilegeEscalation:*false,RunAsGroup:*2001,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod kubernetes-dashboard-7779f9b69b-rgj2t_kubernetes-dashboard(491f05f4-6094-465a-b486-4ee603b518a1): ErrImagePull: rpc error: code = Canceled desc = context canceled" logger="UnhandledError"
Mar 17 18:44:21 minikube kubelet[2213]: E0317 18:44:21.128830    2213 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with ErrImagePull: \"rpc error: code = Canceled desc = context canceled\"" pod="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b-rgj2t" podUID="491f05f4-6094-465a-b486-4ee603b518a1"
Mar 17 18:44:25 minikube kubelet[2213]: E0317 18:44:25.130260    2213 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for yourusername/cpu-utilization, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="yourusername/cpu-utilization:latest"
Mar 17 18:44:25 minikube kubelet[2213]: E0317 18:44:25.130341    2213 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for yourusername/cpu-utilization, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="yourusername/cpu-utilization:latest"
Mar 17 18:44:25 minikube kubelet[2213]: E0317 18:44:25.130526    2213 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:cpu-utilization,Image:yourusername/cpu-utilization:latest,Command:[python /scripts/fetch_cpu_utilization.py],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:cpu-logs,ReadOnly:false,MountPath:/path/to,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-675lz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod cpu-utilization-job-29037280-4854z_default(4fd7038a-07d5-46f5-86f8-9fb33943e840): ErrImagePull: Error response from daemon: pull access denied for yourusername/cpu-utilization, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Mar 17 18:44:25 minikube kubelet[2213]: E0317 18:44:25.132674    2213 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cpu-utilization\" with ErrImagePull: \"Error response from daemon: pull access denied for yourusername/cpu-utilization, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/cpu-utilization-job-29037280-4854z" podUID="4fd7038a-07d5-46f5-86f8-9fb33943e840"
Mar 17 18:44:26 minikube kubelet[2213]: E0317 18:44:26.068253    2213 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cpu-utilization\" with ImagePullBackOff: \"Back-off pulling image \\\"yourusername/cpu-utilization:latest\\\": ErrImagePull: Error response from daemon: pull access denied for yourusername/cpu-utilization, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/cpu-utilization-job-29037280-4854z" podUID="4fd7038a-07d5-46f5-86f8-9fb33943e840"
Mar 17 18:44:27 minikube kubelet[2213]: E0317 18:44:27.589071    2213 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Mar 17 18:44:27 minikube kubelet[2213]: E0317 18:44:27.589146    2213 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/ee9f5ea0-4222-4f34-9a30-ab285d1892da-webhook-cert podName:ee9f5ea0-4222-4f34-9a30-ab285d1892da nodeName:}" failed. No retries permitted until 2025-03-17 18:46:29.589134031 +0000 UTC m=+1042.581617872 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/ee9f5ea0-4222-4f34-9a30-ab285d1892da-webhook-cert") pod "ingress-nginx-controller-56d7c84fd4-ttg2j" (UID: "ee9f5ea0-4222-4f34-9a30-ab285d1892da") : secret "ingress-nginx-admission" not found
Mar 17 18:44:34 minikube kubelet[2213]: E0317 18:44:34.082884    2213 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93\\\": ErrImagePull: rpc error: code = Canceled desc = context canceled\"" pod="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b-rgj2t" podUID="491f05f4-6094-465a-b486-4ee603b518a1"
Mar 17 18:44:37 minikube kubelet[2213]: E0317 18:44:37.082073    2213 pod_workers.go:1301] "Error syncing pod, skipping" err="unmounted volumes=[webhook-cert], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="ingress-nginx/ingress-nginx-controller-56d7c84fd4-ttg2j" podUID="ee9f5ea0-4222-4f34-9a30-ab285d1892da"
Mar 17 18:44:49 minikube kubelet[2213]: E0317 18:44:49.084513    2213 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93\\\": ErrImagePull: rpc error: code = Canceled desc = context canceled\"" pod="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b-rgj2t" podUID="491f05f4-6094-465a-b486-4ee603b518a1"
Mar 17 18:45:00 minikube kubelet[2213]: I0317 18:45:00.203477    2213 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"cpu-logs\" (UniqueName: \"kubernetes.io/host-path/0c5008fe-101f-4ade-af1c-45cdb6e960f4-cpu-logs\") pod \"cpu-utilization-job-29037285-8vlzq\" (UID: \"0c5008fe-101f-4ade-af1c-45cdb6e960f4\") " pod="default/cpu-utilization-job-29037285-8vlzq"
Mar 17 18:45:00 minikube kubelet[2213]: I0317 18:45:00.203530    2213 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-jh9rl\" (UniqueName: \"kubernetes.io/projected/0c5008fe-101f-4ade-af1c-45cdb6e960f4-kube-api-access-jh9rl\") pod \"cpu-utilization-job-29037285-8vlzq\" (UID: \"0c5008fe-101f-4ade-af1c-45cdb6e960f4\") " pod="default/cpu-utilization-job-29037285-8vlzq"
Mar 17 18:45:07 minikube kubelet[2213]: E0317 18:45:07.093396    2213 iptables.go:577] "Could not set up iptables canary" err=<
Mar 17 18:45:07 minikube kubelet[2213]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Mar 17 18:45:07 minikube kubelet[2213]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Mar 17 18:45:07 minikube kubelet[2213]:         Perhaps ip6tables or your kernel needs to be upgraded.
Mar 17 18:45:07 minikube kubelet[2213]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Mar 17 18:45:31 minikube kubelet[2213]: I0317 18:45:31.578606    2213 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-ingress-dns-minikube" podStartSLOduration=4.793465634 podStartE2EDuration="5m20.578585579s" podCreationTimestamp="2025-03-17 18:40:11 +0000 UTC" firstStartedPulling="2025-03-17 18:40:11.895020871 +0000 UTC m=+664.887504705" lastFinishedPulling="2025-03-17 18:45:27.680140808 +0000 UTC m=+980.672624650" observedRunningTime="2025-03-17 18:45:28.356904002 +0000 UTC m=+981.349387856" watchObservedRunningTime="2025-03-17 18:45:31.578585579 +0000 UTC m=+984.571069434"
Mar 17 18:45:31 minikube kubelet[2213]: I0317 18:45:31.725509    2213 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp-volume\" (UniqueName: \"kubernetes.io/empty-dir/1b7b3d16-9a4c-4d15-ba72-83969fcf7eed-tmp-volume\") pod \"kubernetes-dashboard-79cbcf9fb6-s44ht\" (UID: \"1b7b3d16-9a4c-4d15-ba72-83969fcf7eed\") " pod="kubernetes-dashboard/kubernetes-dashboard-79cbcf9fb6-s44ht"
Mar 17 18:45:31 minikube kubelet[2213]: I0317 18:45:31.725628    2213 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubernetes-dashboard-certs\" (UniqueName: \"kubernetes.io/secret/1b7b3d16-9a4c-4d15-ba72-83969fcf7eed-kubernetes-dashboard-certs\") pod \"kubernetes-dashboard-79cbcf9fb6-s44ht\" (UID: \"1b7b3d16-9a4c-4d15-ba72-83969fcf7eed\") " pod="kubernetes-dashboard/kubernetes-dashboard-79cbcf9fb6-s44ht"
Mar 17 18:45:31 minikube kubelet[2213]: I0317 18:45:31.725642    2213 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-5ql7b\" (UniqueName: \"kubernetes.io/projected/1b7b3d16-9a4c-4d15-ba72-83969fcf7eed-kube-api-access-5ql7b\") pod \"kubernetes-dashboard-79cbcf9fb6-s44ht\" (UID: \"1b7b3d16-9a4c-4d15-ba72-83969fcf7eed\") " pod="kubernetes-dashboard/kubernetes-dashboard-79cbcf9fb6-s44ht"
Mar 17 18:45:31 minikube kubelet[2213]: I0317 18:45:31.826098    2213 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp-volume\" (UniqueName: \"kubernetes.io/empty-dir/13f02c10-00fa-4ab1-bd5f-857a5ce679db-tmp-volume\") pod \"dashboard-metrics-scraper-5bd45c9dd6-7r8bk\" (UID: \"13f02c10-00fa-4ab1-bd5f-857a5ce679db\") " pod="kubernetes-dashboard/dashboard-metrics-scraper-5bd45c9dd6-7r8bk"
Mar 17 18:45:31 minikube kubelet[2213]: I0317 18:45:31.826142    2213 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-9grwm\" (UniqueName: \"kubernetes.io/projected/13f02c10-00fa-4ab1-bd5f-857a5ce679db-kube-api-access-9grwm\") pod \"dashboard-metrics-scraper-5bd45c9dd6-7r8bk\" (UID: \"13f02c10-00fa-4ab1-bd5f-857a5ce679db\") " pod="kubernetes-dashboard/dashboard-metrics-scraper-5bd45c9dd6-7r8bk"
Mar 17 18:46:02 minikube kubelet[2213]: I0317 18:46:02.739155    2213 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"kube-api-access-x4fh4\" (UniqueName: \"kubernetes.io/projected/e79aa115-1163-4837-8de3-db64e0201d04-kube-api-access-x4fh4\") pod \"e79aa115-1163-4837-8de3-db64e0201d04\" (UID: \"e79aa115-1163-4837-8de3-db64e0201d04\") "
Mar 17 18:46:02 minikube kubelet[2213]: I0317 18:46:02.739213    2213 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"kube-api-access-rhmr2\" (UniqueName: \"kubernetes.io/projected/a3363dc1-6165-44c0-b6d4-6738b53a70f9-kube-api-access-rhmr2\") pod \"a3363dc1-6165-44c0-b6d4-6738b53a70f9\" (UID: \"a3363dc1-6165-44c0-b6d4-6738b53a70f9\") "
Mar 17 18:46:02 minikube kubelet[2213]: I0317 18:46:02.742929    2213 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/a3363dc1-6165-44c0-b6d4-6738b53a70f9-kube-api-access-rhmr2" (OuterVolumeSpecName: "kube-api-access-rhmr2") pod "a3363dc1-6165-44c0-b6d4-6738b53a70f9" (UID: "a3363dc1-6165-44c0-b6d4-6738b53a70f9"). InnerVolumeSpecName "kube-api-access-rhmr2". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Mar 17 18:46:02 minikube kubelet[2213]: I0317 18:46:02.743258    2213 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/e79aa115-1163-4837-8de3-db64e0201d04-kube-api-access-x4fh4" (OuterVolumeSpecName: "kube-api-access-x4fh4") pod "e79aa115-1163-4837-8de3-db64e0201d04" (UID: "e79aa115-1163-4837-8de3-db64e0201d04"). InnerVolumeSpecName "kube-api-access-x4fh4". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Mar 17 18:46:02 minikube kubelet[2213]: I0317 18:46:02.840093    2213 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-rhmr2\" (UniqueName: \"kubernetes.io/projected/a3363dc1-6165-44c0-b6d4-6738b53a70f9-kube-api-access-rhmr2\") on node \"minikube\" DevicePath \"\""
Mar 17 18:46:02 minikube kubelet[2213]: I0317 18:46:02.840128    2213 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-x4fh4\" (UniqueName: \"kubernetes.io/projected/e79aa115-1163-4837-8de3-db64e0201d04-kube-api-access-x4fh4\") on node \"minikube\" DevicePath \"\""
Mar 17 18:46:03 minikube kubelet[2213]: I0317 18:46:03.539756    2213 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="5f7a12d4d858c183d8ade3c2de10af9683139bc106e4c5325d98755cbaa65abb"
Mar 17 18:46:03 minikube kubelet[2213]: I0317 18:46:03.544599    2213 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="a0cfa6b09eeeeadfa9fba56c4db1df70a87866cbf8b7b34873b2de3196a61108"
Mar 17 18:46:07 minikube kubelet[2213]: E0317 18:46:07.093102    2213 iptables.go:577] "Could not set up iptables canary" err=<
Mar 17 18:46:07 minikube kubelet[2213]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Mar 17 18:46:07 minikube kubelet[2213]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Mar 17 18:46:07 minikube kubelet[2213]:         Perhaps ip6tables or your kernel needs to be upgraded.
Mar 17 18:46:07 minikube kubelet[2213]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Mar 17 18:47:07 minikube kubelet[2213]: E0317 18:47:07.093384    2213 iptables.go:577] "Could not set up iptables canary" err=<
Mar 17 18:47:07 minikube kubelet[2213]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Mar 17 18:47:07 minikube kubelet[2213]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Mar 17 18:47:07 minikube kubelet[2213]:         Perhaps ip6tables or your kernel needs to be upgraded.
Mar 17 18:47:07 minikube kubelet[2213]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Mar 17 18:48:07 minikube kubelet[2213]: E0317 18:48:07.092794    2213 iptables.go:577] "Could not set up iptables canary" err=<
Mar 17 18:48:07 minikube kubelet[2213]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Mar 17 18:48:07 minikube kubelet[2213]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Mar 17 18:48:07 minikube kubelet[2213]:         Perhaps ip6tables or your kernel needs to be upgraded.
Mar 17 18:48:07 minikube kubelet[2213]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Mar 17 18:48:49 minikube kubelet[2213]: E0317 18:48:49.745588    2213 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = error pulling image configuration: download failed after attempts=6: dial tcp 104.16.98.215:443: i/o timeout" image="docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c"
Mar 17 18:48:49 minikube kubelet[2213]: E0317 18:48:49.745648    2213 kuberuntime_image.go:55] "Failed to pull image" err="error pulling image configuration: download failed after attempts=6: dial tcp 104.16.98.215:443: i/o timeout" image="docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c"
Mar 17 18:48:49 minikube kubelet[2213]: E0317 18:48:49.745891    2213 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:dashboard-metrics-scraper,Image:docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:tmp-volume,ReadOnly:false,MountPath:/tmp,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-rfstt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/,Port:{0 8000 },Host:,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:30,TimeoutSeconds:30,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,},ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:*1001,RunAsNonRoot:nil,ReadOnlyRootFilesystem:*true,AllowPrivilegeEscalation:*false,RunAsGroup:*2001,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod dashboard-metrics-scraper-5d59dccf9b-9stv2_kubernetes-dashboard(17edf8c7-5e4d-4f7b-8ef8-c40dfc67dfd1): ErrImagePull: error pulling image configuration: download failed after attempts=6: dial tcp 104.16.98.215:443: i/o timeout" logger="UnhandledError"
Mar 17 18:48:49 minikube kubelet[2213]: E0317 18:48:49.747606    2213 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"dashboard-metrics-scraper\" with ErrImagePull: \"error pulling image configuration: download failed after attempts=6: dial tcp 104.16.98.215:443: i/o timeout\"" pod="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b-9stv2" podUID="17edf8c7-5e4d-4f7b-8ef8-c40dfc67dfd1"
Mar 17 18:49:05 minikube kubelet[2213]: E0317 18:49:05.082364    2213 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"dashboard-metrics-scraper\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c\\\": ErrImagePull: error pulling image configuration: download failed after attempts=6: dial tcp 104.16.98.215:443: i/o timeout\"" pod="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b-9stv2" podUID="17edf8c7-5e4d-4f7b-8ef8-c40dfc67dfd1"
Mar 17 18:49:07 minikube kubelet[2213]: E0317 18:49:07.094550    2213 iptables.go:577] "Could not set up iptables canary" err=<
Mar 17 18:49:07 minikube kubelet[2213]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
Mar 17 18:49:07 minikube kubelet[2213]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Mar 17 18:49:07 minikube kubelet[2213]:         Perhaps ip6tables or your kernel needs to be upgraded.
Mar 17 18:49:07 minikube kubelet[2213]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Mar 17 18:49:17 minikube kubelet[2213]: E0317 18:49:17.083593    2213 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"dashboard-metrics-scraper\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c\\\": ErrImagePull: error pulling image configuration: download failed after attempts=6: dial tcp 104.16.98.215:443: i/o timeout\"" pod="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b-9stv2" podUID="17edf8c7-5e4d-4f7b-8ef8-c40dfc67dfd1"


==> storage-provisioner [0bf9a5626393] <==
I0317 18:29:43.438940       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0317 18:29:43.445009       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0317 18:29:43.445056       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0317 18:29:43.450974       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0317 18:29:43.452102       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_b615650e-9e90-4985-a691-ac19f087d7a6!
I0317 18:29:43.452437       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"ca0b23a3-acf8-4b6d-8982-199ee014e32e", APIVersion:"v1", ResourceVersion:"483", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_b615650e-9e90-4985-a691-ac19f087d7a6 became leader
I0317 18:29:43.553783       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_b615650e-9e90-4985-a691-ac19f087d7a6!


==> storage-provisioner [851579b7d45a] <==
I0317 18:29:12.985974       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0317 18:29:42.987627       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

